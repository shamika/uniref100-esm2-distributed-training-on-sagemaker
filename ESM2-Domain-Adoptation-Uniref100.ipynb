{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb0b8ea4-195e-4481-9bf4-9d5a039f7ea4",
   "metadata": {},
   "source": [
    "# ESM-2 Domain Adaptation with Uniref100 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d1e05a-d104-4eee-aa5a-5f9b8e84f9fd",
   "metadata": {},
   "source": [
    "In this notebook, we demonstrate how to perform full-parameter fine tuning of the ESM-2 protein language model on uniref100 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40914900-f51c-46ef-8f0c-77acacaf2af1",
   "metadata": {},
   "source": [
    "---\n",
    "## 0. Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0972f4bd-a6f4-4fdd-81fd-4ef47d1790d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q --upgrade pip\n",
    "%pip install -q --upgrade sagemaker boto3 awscli datasets boto3 ipywidgets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dde44515-49fc-47c4-951e-b5193b8bb72a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "Assumed SageMaker role is arn:aws:iam::111918798052:role/DevelopmentRole\n",
      "S3 path is s3://sagemaker-us-east-1-111918798052/esm-2-uniref100-benchmarking\n",
      "Experiment name is esm-2-benchmarking-uniref1002023-10-12-19-16-31\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "import os\n",
    "import sagemaker\n",
    "from sagemaker.experiments.run import Run\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from time import strftime\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "boto_session = boto3.session.Session()\n",
    "sagemaker_session = sagemaker.session.Session(boto_session)\n",
    "S3_BUCKET = sagemaker_session.default_bucket()\n",
    "s3 = boto_session.client(\"s3\")\n",
    "sagemaker_client = boto_session.client(\"sagemaker\")\n",
    "sagemaker_execution_role = sagemaker.session.get_execution_role(sagemaker_session)\n",
    "REGION_NAME = sagemaker_session.boto_region_name\n",
    "print(f\"Assumed SageMaker role is {sagemaker_execution_role}\")\n",
    "\n",
    "S3_PREFIX = \"esm-2-uniref100-benchmarking\"\n",
    "S3_PATH = sagemaker.s3.s3_path_join(\"s3://\", S3_BUCKET, S3_PREFIX)\n",
    "print(f\"S3 path is {S3_PATH}\")\n",
    "\n",
    "EXPERIMENT_NAME = f\"esm-2-benchmarking-uniref100\" + strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "print(f\"Experiment name is {EXPERIMENT_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "41652d2e-bdae-4f10-b0ba-02cc8882083d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# MODEL_ID=\"facebook/esm2_t48_15B_UR50D\"\n",
    "# MODEL_ID=\"facebook/esm2_t36_3B_UR50D\"\n",
    "MODEL_ID=\"facebook/esm2_t33_650M_UR50D\"\n",
    "# MODEL_ID=\"facebook/esm2_t30_150M_UR50D\"\n",
    "# MODEL_ID=\"facebook/esm2_t12_35M_UR50D\"\n",
    "# MODEL_ID = \"facebook/esm2_t6_8M_UR50D\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3027983a-e6ef-4a83-ac3f-354f7a2aa248",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Pre-Torkenize the data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf35a80-26bf-423d-bd96-cff0d73d0b11",
   "metadata": {},
   "source": [
    "Torkenized using glue script. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b49980d3-eeb7-4cda-9c61-2b669f73bbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_s3_uri_uniref100 = \"s3://us-east-1-protein-ref-data/uniref100/torkenized-1mb-v3/train/\"\n",
    "test_s3_uri_uniref100 = \"s3://us-east-1-protein-ref-data/uniref100/torkenized-1mb-v3/test/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc1eece-1705-43d0-aebb-8b0b4bb57b61",
   "metadata": {},
   "source": [
    "## 2. Create data map needed for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed5c85f-c93d-4a46-86a7-cf99596ea58e",
   "metadata": {
    "tags": []
   },
   "source": [
    "Create index map of torkenized data using glue script. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f35366-a24c-401b-abc0-5aede2c5a3c9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2.1 (Optional) Get sample data for a sample run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2c0dfaf2-10d9-4353-8d09-5b9b43828824",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://us-east-1-protein-ref-data/uniref100/torkenized-1mb-v3/train/train_index_map/part-00000-930723dc-5f05-4e8c-a356-1e7c9ef1c115-c000.csv to tmp/part-00000-930723dc-5f05-4e8c-a356-1e7c9ef1c115-c000.csv\n",
      "download: s3://us-east-1-protein-ref-data/uniref100/torkenized-1mb-v3/test/test_index_map/part-00000-b42d82f2-3f85-471d-9b41-e43e8aa2a8af-c000.csv to tmp/part-00000-b42d82f2-3f85-471d-9b41-e43e8aa2a8af-c000.csv\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp s3://us-east-1-protein-ref-data/uniref100/torkenized-1mb-v3/train/train_index_map/part-00000-930723dc-5f05-4e8c-a356-1e7c9ef1c115-c000.csv ./tmp/\n",
    "!aws s3 cp s3://us-east-1-protein-ref-data/uniref100/torkenized-1mb-v3/test/test_index_map/part-00000-b42d82f2-3f85-471d-9b41-e43e8aa2a8af-c000.csv ./tmp/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "689bec03-e0a6-4144-9a24-86cd7e8322f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_index_map = pd.read_csv(\"./tmp/part-00000-930723dc-5f05-4e8c-a356-1e7c9ef1c115-c000.csv\")\n",
    "train_index_map[0:1500].to_csv(\"./tmp/sample_train_10.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8462239f-3014-4404-b549-ce032d4193d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_index_map = pd.read_csv(\"./tmp/part-00000-b42d82f2-3f85-471d-9b41-e43e8aa2a8af-c000.csv\")\n",
    "train_index_map[0:250].to_csv(\"./tmp/sample_test_10.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7adccc94-75b6-4e8b-9787-94dde78dc1e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: tmp/sample_train_10.csv to s3://us-east-1-protein-ref-data/uniref100/torkenized-1mb-v3/train/sample_train_index_map/sample_train_10.csv\n",
      "upload: tmp/sample_test_10.csv to s3://us-east-1-protein-ref-data/uniref100/torkenized-1mb-v3/test/sample_test_index_map/sample_test_10.csv\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp ./tmp/sample_train_10.csv s3://us-east-1-protein-ref-data/uniref100/torkenized-1mb-v3/train/sample_train_index_map/\n",
    "!aws s3 cp ./tmp/sample_test_10.csv s3://us-east-1-protein-ref-data/uniref100/torkenized-1mb-v3/test/sample_test_index_map/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c45c05-3f53-4ebb-b510-72b0f7e60e89",
   "metadata": {},
   "source": [
    "## 3. Train on multiple g5.2xlarge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e41ad3-cda7-4e14-906e-ff6f9d1bacfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c9e298a5-d9f1-4d7e-aaea-b4eeed5de33b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "metric_definitions = [\n",
    "    {\"Name\": \"epoch\", \"Regex\": \"Epoch: ([0-9.]*)\"},\n",
    "    {\"Name\": \"step\", \"Regex\": \"Step: ([0-9.]*)\"},\n",
    "    {\"Name\": \"train_loss\", \"Regex\": \"Training Loss: ([0-9.e-]*)\"},\n",
    "    {\"Name\": \"train_perplexity\", \"Regex\": \"Training Perplexity: ([0-9.e-]*)\"},\n",
    "    {\n",
    "        \"Name\": \"train_samples_per_second\",\n",
    "        \"Regex\": \"Training Samples/sec: ([0-9.e-]*)\",\n",
    "    },\n",
    "    {\n",
    "        \"Name\": \"train_tokens_per_second\",\n",
    "        \"Regex\": \"Training Tokens/sec: ([0-9.e-]*)\",\n",
    "    },\n",
    "    {\"Name\": \"eval_loss\", \"Regex\": \"Eval Loss: ([0-9.e-]*)\"},\n",
    "    {\"Name\": \"eval_perplexity\", \"Regex\": \"Eval Perplexity: ([0-9.e-]*)\"},\n",
    "    {\n",
    "        \"Name\": \"eval_samples_per_second\",\n",
    "        \"Regex\": \"Eval Samples/sec: ([0-9.e-]*)\",\n",
    "    },\n",
    "    {\"Name\": \"eval_tokens_per_second\", \"Regex\": \"Eval Tokens/sec: ([0-9.e-]*)\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "bd51bd1f-15b5-4536-bdcb-d735daa1230f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: esm-2-uniref100-2p3dn24-2023-10-13-04-37-07-449\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n",
      "2023-10-13 04:37:07 Starting - Starting the training job...\n",
      "2023-10-13 04:37:23 Starting - Preparing the instances for training............\n",
      "2023-10-13 04:39:19 Downloading - Downloading input data...\n",
      "2023-10-13 04:39:56 Training - Downloading the training image............\n",
      "2023-10-13 04:42:02 Training - Training image download completed. Training in progress.....\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-10-13 04:42:42,615 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-10-13 04:42:42,675 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-10-13 04:42:42,686 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-10-13 04:42:42,688 sagemaker_pytorch_container.training INFO     Invoking TorchDistributed...\u001b[0m\n",
      "\u001b[34m2023-10-13 04:42:42,688 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-10-13 04:42:43,894 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.9 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting transformers==4.31.0 (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for transformers==4.31.0 from https://files.pythonhosted.org/packages/21/02/ae8e595f45b6c8edee07913892b3b41f5f5f273962ad98851dc6a564bbb9/transformers-4.31.0-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.31.0-py3-none-any.whl.metadata (116 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 116.9/116.9 kB 8.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting datasets==2.14.2 (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for datasets==2.14.2 from https://files.pythonhosted.org/packages/46/3e/e1a12ac147ad460f67bd59ce9fede8470be4fa7e5b27dc53ba87135c6c15/datasets-2.14.2-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading datasets-2.14.2-py3-none-any.whl.metadata (19 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: accelerate==0.21.0 in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 3)) (0.21.0)\u001b[0m\n",
      "\u001b[34mCollecting evaluate (from -r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mDownloading evaluate-0.4.0-py3-none-any.whl (81 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 81.4/81.4 kB 25.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting tensorboard (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for tensorboard from https://files.pythonhosted.org/packages/73/a2/66ed644f6ed1562e0285fcd959af17670ea313c8f331c46f79ee77187eb9/tensorboard-2.14.1-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading tensorboard-2.14.1-py3-none-any.whl.metadata (1.7 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers==4.31.0->-r requirements.txt (line 1)) (3.12.2)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub<1.0,>=0.14.1 (from transformers==4.31.0->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for huggingface-hub<1.0,>=0.14.1 from https://files.pythonhosted.org/packages/ef/b5/b6107bd65fa4c96fdf00e4733e2fe5729bb9e5e09997f63074bb43d3ab28/huggingface_hub-0.18.0-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.18.0-py3-none-any.whl.metadata (13 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from transformers==4.31.0->-r requirements.txt (line 1)) (1.23.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from transformers==4.31.0->-r requirements.txt (line 1)) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from transformers==4.31.0->-r requirements.txt (line 1)) (6.0.1)\u001b[0m\n",
      "\u001b[34mCollecting regex!=2019.12.17 (from transformers==4.31.0->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for regex!=2019.12.17 from https://files.pythonhosted.org/packages/54/71/b85c050a8b6a552261e9deae23ba20099852cfbcc9819a628ce64f5a0db6/regex-2023.10.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading regex-2023.10.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.9/40.9 kB 12.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from transformers==4.31.0->-r requirements.txt (line 1)) (2.31.0)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.31.0->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.8/7.8 MB 77.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting safetensors>=0.3.1 (from transformers==4.31.0->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for safetensors>=0.3.1 from https://files.pythonhosted.org/packages/8b/50/e72b8adba500c8d09f768953196378eac2a119717cc90b2b6b14e044ad44/safetensors-0.4.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading safetensors-0.4.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.9/site-packages (from transformers==4.31.0->-r requirements.txt (line 1)) (4.64.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.9/site-packages (from datasets==2.14.2->-r requirements.txt (line 2)) (12.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.9/site-packages (from datasets==2.14.2->-r requirements.txt (line 2)) (0.3.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from datasets==2.14.2->-r requirements.txt (line 2)) (2.0.3)\u001b[0m\n",
      "\u001b[34mCollecting xxhash (from datasets==2.14.2->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for xxhash from https://files.pythonhosted.org/packages/63/93/812d78f70145c68c4e64533f4d625bea01236f27698febe15f0ceebc1566/xxhash-3.4.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading xxhash-3.4.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.9/site-packages (from datasets==2.14.2->-r requirements.txt (line 2)) (0.70.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.9/site-packages (from datasets==2.14.2->-r requirements.txt (line 2)) (2023.6.0)\u001b[0m\n",
      "\u001b[34mCollecting aiohttp (from datasets==2.14.2->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for aiohttp from https://files.pythonhosted.org/packages/34/97/d042349afcad79d7c813b07e34cb3a6f0024b4faf07346509b115fe19f97/aiohttp-3.8.6-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading aiohttp-3.8.6-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.9/site-packages (from accelerate==0.21.0->-r requirements.txt (line 3)) (5.9.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.9/site-packages (from accelerate==0.21.0->-r requirements.txt (line 3)) (1.13.1+cu117)\u001b[0m\n",
      "\u001b[34mCollecting responses<0.19 (from evaluate->-r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mDownloading responses-0.18.0-py3-none-any.whl (38 kB)\u001b[0m\n",
      "\u001b[34mCollecting absl-py>=0.4 (from tensorboard->-r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for absl-py>=0.4 from https://files.pythonhosted.org/packages/01/e4/dc0a1dcc4e74e08d7abedab278c795eef54a224363bb18f5692f416d834f/absl_py-2.0.0-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading absl_py-2.0.0-py3-none-any.whl.metadata (2.3 kB)\u001b[0m\n",
      "\u001b[34mCollecting grpcio>=1.48.2 (from tensorboard->-r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for grpcio>=1.48.2 from https://files.pythonhosted.org/packages/d1/a1/adf44cb808bcda1997d8afb3033b4fd503f6f5e89a6d3eeb454cb84c8abc/grpcio-1.59.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading grpcio-1.59.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\u001b[0m\n",
      "\u001b[34mCollecting google-auth<3,>=1.6.3 (from tensorboard->-r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for google-auth<3,>=1.6.3 from https://files.pythonhosted.org/packages/39/7c/2e4fa55a99f83ef9ef229ac5d59c44ceb90e2d0145711590c0fa39669f32/google_auth-2.23.3-py2.py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading google_auth-2.23.3-py2.py3-none-any.whl.metadata (4.2 kB)\u001b[0m\n",
      "\u001b[34mCollecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard->-r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mDownloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\u001b[0m\n",
      "\u001b[34mCollecting markdown>=2.6.8 (from tensorboard->-r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for markdown>=2.6.8 from https://files.pythonhosted.org/packages/bb/c1/50caaec6cadc1c6adc8fe351e03bd646d6e4dd17f55fca0f4c8d7ea8d3e9/Markdown-3.5-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading Markdown-3.5-py3-none-any.whl.metadata (7.1 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf>=3.19.6 in /opt/conda/lib/python3.9/site-packages (from tensorboard->-r requirements.txt (line 5)) (3.20.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.9/site-packages (from tensorboard->-r requirements.txt (line 5)) (65.6.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>1.9 in /opt/conda/lib/python3.9/site-packages (from tensorboard->-r requirements.txt (line 5)) (1.16.0)\u001b[0m\n",
      "\u001b[34mCollecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard->-r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for tensorboard-data-server<0.8.0,>=0.7.0 from https://files.pythonhosted.org/packages/02/52/fb9e51fba47951aabd7a6b25e41d73eae94208ccf62d886168096941a781/tensorboard_data_server-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading tensorboard_data_server-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (1.1 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.9/site-packages (from tensorboard->-r requirements.txt (line 5)) (2.3.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets==2.14.2->-r requirements.txt (line 2)) (23.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets==2.14.2->-r requirements.txt (line 2)) (3.2.0)\u001b[0m\n",
      "\u001b[35mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[35mbash: no job control in this shell\u001b[0m\n",
      "\u001b[35m2023-10-13 04:42:46,349 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[35m2023-10-13 04:42:46,409 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2023-10-13 04:42:46,420 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[35m2023-10-13 04:42:46,422 sagemaker_pytorch_container.training INFO     Invoking TorchDistributed...\u001b[0m\n",
      "\u001b[35m2023-10-13 04:42:46,422 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[35m2023-10-13 04:42:47,593 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[35m/opt/conda/bin/python3.9 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[35mCollecting transformers==4.31.0 (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for transformers==4.31.0 from https://files.pythonhosted.org/packages/21/02/ae8e595f45b6c8edee07913892b3b41f5f5f273962ad98851dc6a564bbb9/transformers-4.31.0-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading transformers-4.31.0-py3-none-any.whl.metadata (116 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 116.9/116.9 kB 8.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting datasets==2.14.2 (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for datasets==2.14.2 from https://files.pythonhosted.org/packages/46/3e/e1a12ac147ad460f67bd59ce9fede8470be4fa7e5b27dc53ba87135c6c15/datasets-2.14.2-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading datasets-2.14.2-py3-none-any.whl.metadata (19 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: accelerate==0.21.0 in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 3)) (0.21.0)\u001b[0m\n",
      "\u001b[35mCollecting evaluate (from -r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[35mDownloading evaluate-0.4.0-py3-none-any.whl (81 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 81.4/81.4 kB 22.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting tensorboard (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for tensorboard from https://files.pythonhosted.org/packages/73/a2/66ed644f6ed1562e0285fcd959af17670ea313c8f331c46f79ee77187eb9/tensorboard-2.14.1-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading tensorboard-2.14.1-py3-none-any.whl.metadata (1.7 kB)\u001b[0m\n",
      "\u001b[34mCollecting multidict<7.0,>=4.5 (from aiohttp->datasets==2.14.2->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 114.2/114.2 kB 36.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets==2.14.2->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for async-timeout<5.0,>=4.0.0a3 from https://files.pythonhosted.org/packages/a7/fa/e01228c2938de91d47b307831c62ab9e4001e747789d0b05baf779a6488c/async_timeout-4.0.3-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\u001b[0m\n",
      "\u001b[34mCollecting yarl<2.0,>=1.0 (from aiohttp->datasets==2.14.2->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading yarl-1.9.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (269 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 269.4/269.4 kB 56.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting frozenlist>=1.1.1 (from aiohttp->datasets==2.14.2->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for frozenlist>=1.1.1 from https://files.pythonhosted.org/packages/b5/03/7dec2e257bd173b5ca1f74477863b97d322149f6f0284d7decead8c5ceeb/frozenlist-1.4.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading frozenlist-1.4.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\u001b[0m\n",
      "\u001b[34mCollecting aiosignal>=1.1.2 (from aiohttp->datasets==2.14.2->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\u001b[0m\n",
      "\u001b[34mCollecting cachetools<6.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for cachetools<6.0,>=2.0.0 from https://files.pythonhosted.org/packages/a9/c9/c8a7710f2cedcb1db9224fdd4d8307c9e48cbddc46c18b515fefc0f1abbe/cachetools-5.3.1-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading cachetools-5.3.1-py3-none-any.whl.metadata (5.2 kB)\u001b[0m\n",
      "\u001b[34mCollecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mDownloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 181.3/181.3 kB 43.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 5)) (4.7.2)\u001b[0m\n",
      "\u001b[34mCollecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<1.1,>=0.5->tensorboard->-r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mDownloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0->-r requirements.txt (line 1)) (4.7.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard->-r requirements.txt (line 5)) (6.8.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.31.0->-r requirements.txt (line 1)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.31.0->-r requirements.txt (line 1)) (1.26.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.31.0->-r requirements.txt (line 1)) (2023.7.22)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.9/site-packages (from werkzeug>=1.0.1->tensorboard->-r requirements.txt (line 5)) (2.1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets==2.14.2->-r requirements.txt (line 2)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets==2.14.2->-r requirements.txt (line 2)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets==2.14.2->-r requirements.txt (line 2)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->-r requirements.txt (line 5)) (3.16.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 5)) (0.5.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers==4.31.0->-r requirements.txt (line 1)) (3.12.2)\u001b[0m\n",
      "\u001b[35mCollecting huggingface-hub<1.0,>=0.14.1 (from transformers==4.31.0->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for huggingface-hub<1.0,>=0.14.1 from https://files.pythonhosted.org/packages/ef/b5/b6107bd65fa4c96fdf00e4733e2fe5729bb9e5e09997f63074bb43d3ab28/huggingface_hub-0.18.0-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading huggingface_hub-0.18.0-py3-none-any.whl.metadata (13 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from transformers==4.31.0->-r requirements.txt (line 1)) (1.23.5)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from transformers==4.31.0->-r requirements.txt (line 1)) (23.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from transformers==4.31.0->-r requirements.txt (line 1)) (6.0.1)\u001b[0m\n",
      "\u001b[35mCollecting regex!=2019.12.17 (from transformers==4.31.0->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for regex!=2019.12.17 from https://files.pythonhosted.org/packages/54/71/b85c050a8b6a552261e9deae23ba20099852cfbcc9819a628ce64f5a0db6/regex-2023.10.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading regex-2023.10.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.9/40.9 kB 12.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from transformers==4.31.0->-r requirements.txt (line 1)) (2.31.0)\u001b[0m\n",
      "\u001b[34mCollecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard->-r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mDownloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 151.7/151.7 kB 43.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.4/7.4 MB 107.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading datasets-2.14.2-py3-none-any.whl (518 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 518.9/518.9 kB 68.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading tensorboard-2.14.1-py3-none-any.whl (5.5 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.5/5.5 MB 121.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading absl_py-2.0.0-py3-none-any.whl (130 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 130.2/130.2 kB 36.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading aiohttp-3.8.6-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 87.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading google_auth-2.23.3-py2.py3-none-any.whl (182 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 182.3/182.3 kB 48.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading grpcio-1.59.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.3/5.3 MB 119.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 302.0/302.0 kB 52.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading Markdown-3.5-py3-none-any.whl (101 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 101.7/101.7 kB 19.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading regex-2023.10.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 773.3/773.3 kB 65.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading safetensors-0.4.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 101.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading tensorboard_data_server-0.7.1-py3-none-manylinux2014_x86_64.whl (6.6 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.6/6.6 MB 101.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading xxhash-3.4.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (193 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 193.8/193.8 kB 42.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\u001b[0m\n",
      "\u001b[34mDownloading cachetools-5.3.1-py3-none-any.whl (9.3 kB)\u001b[0m\n",
      "\u001b[34mDownloading frozenlist-1.4.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (228 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 228.0/228.0 kB 52.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.31.0->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[35mDownloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.8/7.8 MB 83.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting safetensors>=0.3.1 (from transformers==4.31.0->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for safetensors>=0.3.1 from https://files.pythonhosted.org/packages/8b/50/e72b8adba500c8d09f768953196378eac2a119717cc90b2b6b14e044ad44/safetensors-0.4.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading safetensors-0.4.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.9/site-packages (from transformers==4.31.0->-r requirements.txt (line 1)) (4.64.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.9/site-packages (from datasets==2.14.2->-r requirements.txt (line 2)) (12.0.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.9/site-packages (from datasets==2.14.2->-r requirements.txt (line 2)) (0.3.7)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from datasets==2.14.2->-r requirements.txt (line 2)) (2.0.3)\u001b[0m\n",
      "\u001b[35mCollecting xxhash (from datasets==2.14.2->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for xxhash from https://files.pythonhosted.org/packages/63/93/812d78f70145c68c4e64533f4d625bea01236f27698febe15f0ceebc1566/xxhash-3.4.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading xxhash-3.4.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.9/site-packages (from datasets==2.14.2->-r requirements.txt (line 2)) (0.70.15)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.9/site-packages (from datasets==2.14.2->-r requirements.txt (line 2)) (2023.6.0)\u001b[0m\n",
      "\u001b[35mCollecting aiohttp (from datasets==2.14.2->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for aiohttp from https://files.pythonhosted.org/packages/34/97/d042349afcad79d7c813b07e34cb3a6f0024b4faf07346509b115fe19f97/aiohttp-3.8.6-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading aiohttp-3.8.6-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: psutil in /opt/conda/lib/python3.9/site-packages (from accelerate==0.21.0->-r requirements.txt (line 3)) (5.9.5)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.9/site-packages (from accelerate==0.21.0->-r requirements.txt (line 3)) (1.13.1+cu117)\u001b[0m\n",
      "\u001b[35mCollecting responses<0.19 (from evaluate->-r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[35mDownloading responses-0.18.0-py3-none-any.whl (38 kB)\u001b[0m\n",
      "\u001b[35mCollecting absl-py>=0.4 (from tensorboard->-r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for absl-py>=0.4 from https://files.pythonhosted.org/packages/01/e4/dc0a1dcc4e74e08d7abedab278c795eef54a224363bb18f5692f416d834f/absl_py-2.0.0-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading absl_py-2.0.0-py3-none-any.whl.metadata (2.3 kB)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: tokenizers, xxhash, tensorboard-data-server, safetensors, regex, pyasn1-modules, oauthlib, multidict, grpcio, frozenlist, cachetools, async-timeout, absl-py, yarl, responses, requests-oauthlib, markdown, huggingface-hub, google-auth, aiosignal, transformers, google-auth-oauthlib, aiohttp, tensorboard, datasets, evaluate\u001b[0m\n",
      "\u001b[35mCollecting grpcio>=1.48.2 (from tensorboard->-r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for grpcio>=1.48.2 from https://files.pythonhosted.org/packages/d1/a1/adf44cb808bcda1997d8afb3033b4fd503f6f5e89a6d3eeb454cb84c8abc/grpcio-1.59.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading grpcio-1.59.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\u001b[0m\n",
      "\u001b[35mCollecting google-auth<3,>=1.6.3 (from tensorboard->-r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for google-auth<3,>=1.6.3 from https://files.pythonhosted.org/packages/39/7c/2e4fa55a99f83ef9ef229ac5d59c44ceb90e2d0145711590c0fa39669f32/google_auth-2.23.3-py2.py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading google_auth-2.23.3-py2.py3-none-any.whl.metadata (4.2 kB)\u001b[0m\n",
      "\u001b[35mCollecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard->-r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[35mDownloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\u001b[0m\n",
      "\u001b[35mCollecting markdown>=2.6.8 (from tensorboard->-r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for markdown>=2.6.8 from https://files.pythonhosted.org/packages/bb/c1/50caaec6cadc1c6adc8fe351e03bd646d6e4dd17f55fca0f4c8d7ea8d3e9/Markdown-3.5-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading Markdown-3.5-py3-none-any.whl.metadata (7.1 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: protobuf>=3.19.6 in /opt/conda/lib/python3.9/site-packages (from tensorboard->-r requirements.txt (line 5)) (3.20.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.9/site-packages (from tensorboard->-r requirements.txt (line 5)) (65.6.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: six>1.9 in /opt/conda/lib/python3.9/site-packages (from tensorboard->-r requirements.txt (line 5)) (1.16.0)\u001b[0m\n",
      "\u001b[35mCollecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard->-r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for tensorboard-data-server<0.8.0,>=0.7.0 from https://files.pythonhosted.org/packages/02/52/fb9e51fba47951aabd7a6b25e41d73eae94208ccf62d886168096941a781/tensorboard_data_server-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading tensorboard_data_server-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (1.1 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.9/site-packages (from tensorboard->-r requirements.txt (line 5)) (2.3.6)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets==2.14.2->-r requirements.txt (line 2)) (23.1.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets==2.14.2->-r requirements.txt (line 2)) (3.2.0)\u001b[0m\n",
      "\u001b[35mCollecting multidict<7.0,>=4.5 (from aiohttp->datasets==2.14.2->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[35mDownloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 114.2/114.2 kB 27.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets==2.14.2->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for async-timeout<5.0,>=4.0.0a3 from https://files.pythonhosted.org/packages/a7/fa/e01228c2938de91d47b307831c62ab9e4001e747789d0b05baf779a6488c/async_timeout-4.0.3-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\u001b[0m\n",
      "\u001b[35mCollecting yarl<2.0,>=1.0 (from aiohttp->datasets==2.14.2->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[35mDownloading yarl-1.9.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (269 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 269.4/269.4 kB 56.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting frozenlist>=1.1.1 (from aiohttp->datasets==2.14.2->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for frozenlist>=1.1.1 from https://files.pythonhosted.org/packages/b5/03/7dec2e257bd173b5ca1f74477863b97d322149f6f0284d7decead8c5ceeb/frozenlist-1.4.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading frozenlist-1.4.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\u001b[0m\n",
      "\u001b[35mCollecting aiosignal>=1.1.2 (from aiohttp->datasets==2.14.2->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[35mDownloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\u001b[0m\n",
      "\u001b[35mCollecting cachetools<6.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for cachetools<6.0,>=2.0.0 from https://files.pythonhosted.org/packages/a9/c9/c8a7710f2cedcb1db9224fdd4d8307c9e48cbddc46c18b515fefc0f1abbe/cachetools-5.3.1-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading cachetools-5.3.1-py3-none-any.whl.metadata (5.2 kB)\u001b[0m\n",
      "\u001b[35mCollecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[35mDownloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 181.3/181.3 kB 38.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 5)) (4.7.2)\u001b[0m\n",
      "\u001b[35mCollecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<1.1,>=0.5->tensorboard->-r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[35mDownloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0->-r requirements.txt (line 1)) (4.7.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard->-r requirements.txt (line 5)) (6.8.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.31.0->-r requirements.txt (line 1)) (3.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.31.0->-r requirements.txt (line 1)) (1.26.14)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.31.0->-r requirements.txt (line 1)) (2023.7.22)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.9/site-packages (from werkzeug>=1.0.1->tensorboard->-r requirements.txt (line 5)) (2.1.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets==2.14.2->-r requirements.txt (line 2)) (2.8.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets==2.14.2->-r requirements.txt (line 2)) (2023.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets==2.14.2->-r requirements.txt (line 2)) (2023.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->-r requirements.txt (line 5)) (3.16.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 5)) (0.5.0)\u001b[0m\n",
      "\u001b[35mCollecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard->-r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[35mDownloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 151.7/151.7 kB 41.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.4/7.4 MB 114.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading datasets-2.14.2-py3-none-any.whl (518 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 518.9/518.9 kB 71.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading tensorboard-2.14.1-py3-none-any.whl (5.5 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.5/5.5 MB 122.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading absl_py-2.0.0-py3-none-any.whl (130 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 130.2/130.2 kB 38.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading aiohttp-3.8.6-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 99.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading google_auth-2.23.3-py2.py3-none-any.whl (182 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 182.3/182.3 kB 50.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading grpcio-1.59.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.3/5.3 MB 127.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 302.0/302.0 kB 54.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading Markdown-3.5-py3-none-any.whl (101 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 101.7/101.7 kB 36.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading regex-2023.10.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 773.3/773.3 kB 80.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading safetensors-0.4.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 39.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading tensorboard_data_server-0.7.1-py3-none-manylinux2014_x86_64.whl (6.6 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.6/6.6 MB 129.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading xxhash-3.4.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (193 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 193.8/193.8 kB 40.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\u001b[0m\n",
      "\u001b[35mDownloading cachetools-5.3.1-py3-none-any.whl (9.3 kB)\u001b[0m\n",
      "\u001b[35mDownloading frozenlist-1.4.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (228 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 228.0/228.0 kB 49.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mInstalling collected packages: tokenizers, xxhash, tensorboard-data-server, safetensors, regex, pyasn1-modules, oauthlib, multidict, grpcio, frozenlist, cachetools, async-timeout, absl-py, yarl, responses, requests-oauthlib, markdown, huggingface-hub, google-auth, aiosignal, transformers, google-auth-oauthlib, aiohttp, tensorboard, datasets, evaluate\u001b[0m\n",
      "\u001b[34mSuccessfully installed absl-py-2.0.0 aiohttp-3.8.6 aiosignal-1.3.1 async-timeout-4.0.3 cachetools-5.3.1 datasets-2.14.2 evaluate-0.4.0 frozenlist-1.4.0 google-auth-2.23.3 google-auth-oauthlib-1.0.0 grpcio-1.59.0 huggingface-hub-0.18.0 markdown-3.5 multidict-6.0.4 oauthlib-3.2.2 pyasn1-modules-0.3.0 regex-2023.10.3 requests-oauthlib-1.3.1 responses-0.18.0 safetensors-0.4.0 tensorboard-2.14.1 tensorboard-data-server-0.7.1 tokenizers-0.13.3 transformers-4.31.0 xxhash-3.4.1 yarl-1.9.2\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2023-10-13 04:42:57,923 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-10-13 04:42:57,923 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-10-13 04:42:57,994 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-10-13 04:42:58,074 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-10-13 04:42:58,088 sagemaker-training-toolkit INFO     Starting distributed training through torchrun\u001b[0m\n",
      "\u001b[34m2023-10-13 04:42:58,153 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-10-13 04:42:58,168 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_instance_type\": \"ml.p3dn.24xlarge\",\n",
      "        \"sagemaker_torch_distributed_enabled\": true\n",
      "    },\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p3dn.24xlarge\",\n",
      "    \"distribution_hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"distribution_instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"bf16\": true,\n",
      "        \"logging_steps\": 8,\n",
      "        \"model_id\": \"facebook/esm2_t33_650M_UR50D\",\n",
      "        \"num_train_epochs\": 2,\n",
      "        \"optim\": \"adamw_torch\",\n",
      "        \"per_device_eval_batch_size\": 12,\n",
      "        \"per_device_train_batch_size\": 12,\n",
      "        \"pretrain\": 1,\n",
      "        \"test_index_file_path\": \"sample_test_index_map\",\n",
      "        \"train_index_file_path\": \"sample_train_index_map\",\n",
      "        \"train_sample_count\": 10000\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p3dn.24xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\",\n",
      "                \"algo-2\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"esm-2-uniref100-2p3dn24-2023-10-13-04-37-07-449\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-111918798052/esm-2-uniref100-2p3dn24-2023-10-13-04-37-07-449/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"cuda-uniref100-pretorkenized-mlm-train-ddp-fsdp\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 96,\n",
      "    \"num_gpus\": 8,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p3dn.24xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p3dn.24xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\",\n",
      "                    \"algo-2\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"cuda-uniref100-pretorkenized-mlm-train-ddp-fsdp.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"bf16\":true,\"logging_steps\":8,\"model_id\":\"facebook/esm2_t33_650M_UR50D\",\"num_train_epochs\":2,\"optim\":\"adamw_torch\",\"per_device_eval_batch_size\":12,\"per_device_train_batch_size\":12,\"pretrain\":1,\"test_index_file_path\":\"sample_test_index_map\",\"train_index_file_path\":\"sample_train_index_map\",\"train_sample_count\":10000}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=cuda-uniref100-pretorkenized-mlm-train-ddp-fsdp.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={\"sagemaker_instance_type\":\"ml.p3dn.24xlarge\",\"sagemaker_torch_distributed_enabled\":true}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3dn.24xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3dn.24xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"test\",\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.p3dn.24xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3dn.24xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=cuda-uniref100-pretorkenized-mlm-train-ddp-fsdp\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=96\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-111918798052/esm-2-uniref100-2p3dn24-2023-10-13-04-37-07-449/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_instance_type\":\"ml.p3dn.24xlarge\",\"sagemaker_torch_distributed_enabled\":true},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\",\"algo-2\"],\"current_instance_type\":\"ml.p3dn.24xlarge\",\"distribution_hosts\":[\"algo-1\",\"algo-2\"],\"distribution_instance_groups\":[\"homogeneousCluster\"],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"bf16\":true,\"logging_steps\":8,\"model_id\":\"facebook/esm2_t33_650M_UR50D\",\"num_train_epochs\":2,\"optim\":\"adamw_torch\",\"per_device_eval_batch_size\":12,\"per_device_train_batch_size\":12,\"pretrain\":1,\"test_index_file_path\":\"sample_test_index_map\",\"train_index_file_path\":\"sample_train_index_map\",\"train_sample_count\":10000},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3dn.24xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"esm-2-uniref100-2p3dn24-2023-10-13-04-37-07-449\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-111918798052/esm-2-uniref100-2p3dn24-2023-10-13-04-37-07-449/source/sourcedir.tar.gz\",\"module_name\":\"cuda-uniref100-pretorkenized-mlm-train-ddp-fsdp\",\"network_interface_name\":\"eth0\",\"num_cpus\":96,\"num_gpus\":8,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3dn.24xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3dn.24xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"cuda-uniref100-pretorkenized-mlm-train-ddp-fsdp.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--bf16\",\"True\",\"--logging_steps\",\"8\",\"--model_id\",\"facebook/esm2_t33_650M_UR50D\",\"--num_train_epochs\",\"2\",\"--optim\",\"adamw_torch\",\"--per_device_eval_batch_size\",\"12\",\"--per_device_train_batch_size\",\"12\",\"--pretrain\",\"1\",\"--test_index_file_path\",\"sample_test_index_map\",\"--train_index_file_path\",\"sample_train_index_map\",\"--train_sample_count\",\"10000\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_BF16=true\u001b[0m\n",
      "\u001b[34mSM_HP_LOGGING_STEPS=8\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_ID=facebook/esm2_t33_650M_UR50D\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_TRAIN_EPOCHS=2\u001b[0m\n",
      "\u001b[34mSM_HP_OPTIM=adamw_torch\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_EVAL_BATCH_SIZE=12\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=12\u001b[0m\n",
      "\u001b[34mSM_HP_PRETRAIN=1\u001b[0m\n",
      "\u001b[34mSM_HP_TEST_INDEX_FILE_PATH=sample_test_index_map\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_INDEX_FILE_PATH=sample_train_index_map\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_SAMPLE_COUNT=10000\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python39.zip:/opt/conda/lib/python3.9:/opt/conda/lib/python3.9/lib-dynload:/opt/conda/lib/python3.9/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34mtorchrun --nnodes 2 --nproc_per_node 8 --master_addr algo-1 --master_port 7777 --node_rank 0 cuda-uniref100-pretorkenized-mlm-train-ddp-fsdp.py --bf16 True --logging_steps 8 --model_id facebook/esm2_t33_650M_UR50D --num_train_epochs 2 --optim adamw_torch --per_device_eval_batch_size 12 --per_device_train_batch_size 12 --pretrain 1 --test_index_file_path sample_test_index_map --train_index_file_path sample_train_index_map --train_sample_count 10000\u001b[0m\n",
      "\u001b[34mWARNING:torch.distributed.run:\u001b[0m\n",
      "\u001b[34m*****************************************\u001b[0m\n",
      "\u001b[34mSetting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \u001b[0m\n",
      "\u001b[34m*****************************************\u001b[0m\n",
      "\u001b[35mSuccessfully installed absl-py-2.0.0 aiohttp-3.8.6 aiosignal-1.3.1 async-timeout-4.0.3 cachetools-5.3.1 datasets-2.14.2 evaluate-0.4.0 frozenlist-1.4.0 google-auth-2.23.3 google-auth-oauthlib-1.0.0 grpcio-1.59.0 huggingface-hub-0.18.0 markdown-3.5 multidict-6.0.4 oauthlib-3.2.2 pyasn1-modules-0.3.0 regex-2023.10.3 requests-oauthlib-1.3.1 responses-0.18.0 safetensors-0.4.0 tensorboard-2.14.1 tensorboard-data-server-0.7.1 tokenizers-0.13.3 transformers-4.31.0 xxhash-3.4.1 yarl-1.9.2\u001b[0m\n",
      "\u001b[35mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[35m2023-10-13 04:43:01,746 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[35m2023-10-13 04:43:01,746 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[35m2023-10-13 04:43:01,816 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2023-10-13 04:43:01,896 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2023-10-13 04:43:01,910 sagemaker-training-toolkit INFO     Starting distributed training through torchrun\u001b[0m\n",
      "\u001b[35m2023-10-13 04:43:01,979 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2023-10-13 04:43:01,995 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[35mTraining Env:\u001b[0m\n",
      "\u001b[35m{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_instance_type\": \"ml.p3dn.24xlarge\",\n",
      "        \"sagemaker_torch_distributed_enabled\": true\n",
      "    },\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-2\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p3dn.24xlarge\",\n",
      "    \"distribution_hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"distribution_instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"bf16\": true,\n",
      "        \"logging_steps\": 8,\n",
      "        \"model_id\": \"facebook/esm2_t33_650M_UR50D\",\n",
      "        \"num_train_epochs\": 2,\n",
      "        \"optim\": \"adamw_torch\",\n",
      "        \"per_device_eval_batch_size\": 12,\n",
      "        \"per_device_train_batch_size\": 12,\n",
      "        \"pretrain\": 1,\n",
      "        \"test_index_file_path\": \"sample_test_index_map\",\n",
      "        \"train_index_file_path\": \"sample_train_index_map\",\n",
      "        \"train_sample_count\": 10000\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p3dn.24xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\",\n",
      "                \"algo-2\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": false,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"esm-2-uniref100-2p3dn24-2023-10-13-04-37-07-449\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-111918798052/esm-2-uniref100-2p3dn24-2023-10-13-04-37-07-449/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"cuda-uniref100-pretorkenized-mlm-train-ddp-fsdp\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 96,\n",
      "    \"num_gpus\": 8,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-2\",\n",
      "        \"current_instance_type\": \"ml.p3dn.24xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p3dn.24xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\",\n",
      "                    \"algo-2\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"cuda-uniref100-pretorkenized-mlm-train-ddp-fsdp.py\"\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[35mEnvironment variables:\u001b[0m\n",
      "\u001b[35mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[35mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[35mSM_HPS={\"bf16\":true,\"logging_steps\":8,\"model_id\":\"facebook/esm2_t33_650M_UR50D\",\"num_train_epochs\":2,\"optim\":\"adamw_torch\",\"per_device_eval_batch_size\":12,\"per_device_train_batch_size\":12,\"pretrain\":1,\"test_index_file_path\":\"sample_test_index_map\",\"train_index_file_path\":\"sample_train_index_map\",\"train_sample_count\":10000}\u001b[0m\n",
      "\u001b[35mSM_USER_ENTRY_POINT=cuda-uniref100-pretorkenized-mlm-train-ddp-fsdp.py\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_PARAMS={\"sagemaker_instance_type\":\"ml.p3dn.24xlarge\",\"sagemaker_torch_distributed_enabled\":true}\u001b[0m\n",
      "\u001b[35mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-2\",\"current_instance_type\":\"ml.p3dn.24xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3dn.24xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[35mSM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[35mSM_CHANNELS=[\"test\",\"train\"]\u001b[0m\n",
      "\u001b[35mSM_CURRENT_HOST=algo-2\u001b[0m\n",
      "\u001b[35mSM_CURRENT_INSTANCE_TYPE=ml.p3dn.24xlarge\u001b[0m\n",
      "\u001b[35mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[35mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[35mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[35mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3dn.24xlarge\"}}\u001b[0m\n",
      "\u001b[35mSM_DISTRIBUTION_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[35mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[35mSM_MODULE_NAME=cuda-uniref100-pretorkenized-mlm-train-ddp-fsdp\u001b[0m\n",
      "\u001b[35mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[35mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[35mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[35mSM_NUM_CPUS=96\u001b[0m\n",
      "\u001b[35mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[35mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[35mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[35mSM_MODULE_DIR=s3://sagemaker-us-east-1-111918798052/esm-2-uniref100-2p3dn24-2023-10-13-04-37-07-449/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[35mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_instance_type\":\"ml.p3dn.24xlarge\",\"sagemaker_torch_distributed_enabled\":true},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-2\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\",\"algo-2\"],\"current_instance_type\":\"ml.p3dn.24xlarge\",\"distribution_hosts\":[\"algo-1\",\"algo-2\"],\"distribution_instance_groups\":[\"homogeneousCluster\"],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"bf16\":true,\"logging_steps\":8,\"model_id\":\"facebook/esm2_t33_650M_UR50D\",\"num_train_epochs\":2,\"optim\":\"adamw_torch\",\"per_device_eval_batch_size\":12,\"per_device_train_batch_size\":12,\"pretrain\":1,\"test_index_file_path\":\"sample_test_index_map\",\"train_index_file_path\":\"sample_train_index_map\",\"train_sample_count\":10000},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3dn.24xlarge\"}},\"is_hetero\":false,\"is_master\":false,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"esm-2-uniref100-2p3dn24-2023-10-13-04-37-07-449\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-111918798052/esm-2-uniref100-2p3dn24-2023-10-13-04-37-07-449/source/sourcedir.tar.gz\",\"module_name\":\"cuda-uniref100-pretorkenized-mlm-train-ddp-fsdp\",\"network_interface_name\":\"eth0\",\"num_cpus\":96,\"num_gpus\":8,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-2\",\"current_instance_type\":\"ml.p3dn.24xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3dn.24xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"cuda-uniref100-pretorkenized-mlm-train-ddp-fsdp.py\"}\u001b[0m\n",
      "\u001b[35mSM_USER_ARGS=[\"--bf16\",\"True\",\"--logging_steps\",\"8\",\"--model_id\",\"facebook/esm2_t33_650M_UR50D\",\"--num_train_epochs\",\"2\",\"--optim\",\"adamw_torch\",\"--per_device_eval_batch_size\",\"12\",\"--per_device_train_batch_size\",\"12\",\"--pretrain\",\"1\",\"--test_index_file_path\",\"sample_test_index_map\",\"--train_index_file_path\",\"sample_train_index_map\",\"--train_sample_count\",\"10000\"]\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[35mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[35mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[35mSM_HP_BF16=true\u001b[0m\n",
      "\u001b[35mSM_HP_LOGGING_STEPS=8\u001b[0m\n",
      "\u001b[35mSM_HP_MODEL_ID=facebook/esm2_t33_650M_UR50D\u001b[0m\n",
      "\u001b[35mSM_HP_NUM_TRAIN_EPOCHS=2\u001b[0m\n",
      "\u001b[35mSM_HP_OPTIM=adamw_torch\u001b[0m\n",
      "\u001b[35mSM_HP_PER_DEVICE_EVAL_BATCH_SIZE=12\u001b[0m\n",
      "\u001b[35mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=12\u001b[0m\n",
      "\u001b[35mSM_HP_PRETRAIN=1\u001b[0m\n",
      "\u001b[35mSM_HP_TEST_INDEX_FILE_PATH=sample_test_index_map\u001b[0m\n",
      "\u001b[35mSM_HP_TRAIN_INDEX_FILE_PATH=sample_train_index_map\u001b[0m\n",
      "\u001b[35mSM_HP_TRAIN_SAMPLE_COUNT=10000\u001b[0m\n",
      "\u001b[35mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python39.zip:/opt/conda/lib/python3.9:/opt/conda/lib/python3.9/lib-dynload:/opt/conda/lib/python3.9/site-packages\u001b[0m\n",
      "\u001b[35mInvoking script with the following command:\u001b[0m\n",
      "\u001b[35mtorchrun --nnodes 2 --nproc_per_node 8 --master_addr algo-1 --master_port 7777 --node_rank 1 cuda-uniref100-pretorkenized-mlm-train-ddp-fsdp.py --bf16 True --logging_steps 8 --model_id facebook/esm2_t33_650M_UR50D --num_train_epochs 2 --optim adamw_torch --per_device_eval_batch_size 12 --per_device_train_batch_size 12 --pretrain 1 --test_index_file_path sample_test_index_map --train_index_file_path sample_train_index_map --train_sample_count 10000\u001b[0m\n",
      "\u001b[35mWARNING:torch.distributed.run:\u001b[0m\n",
      "\u001b[35m*****************************************\u001b[0m\n",
      "\u001b[35mSetting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \u001b[0m\n",
      "\u001b[35m*****************************************\u001b[0m\n",
      "\u001b[34mIndex file folder is [/opt/ml/input/data/train/sample_train_index_map]\u001b[0m\n",
      "\u001b[34mIndex file folder is [/opt/ml/input/data/train/sample_train_index_map]\u001b[0m\n",
      "\u001b[34mIndex file folder is [/opt/ml/input/data/train/sample_train_index_map]Index file folder is [/opt/ml/input/data/train/sample_train_index_map]\u001b[0m\n",
      "\u001b[34mIndex file folder is [/opt/ml/input/data/train/sample_train_index_map]\u001b[0m\n",
      "\u001b[34mIndex file folder is [/opt/ml/input/data/train/sample_train_index_map]\u001b[0m\n",
      "\u001b[34mIndex file folder is [/opt/ml/input/data/train/sample_train_index_map]\u001b[0m\n",
      "\u001b[34mIndex file location is [/opt/ml/input/data/train/sample_train_index_map/sample_train_10.csv]Index file location is [/opt/ml/input/data/train/sample_train_index_map/sample_train_10.csv]Index file location is [/opt/ml/input/data/train/sample_train_index_map/sample_train_10.csv]\u001b[0m\n",
      "\u001b[34mIndex file location is [/opt/ml/input/data/train/sample_train_index_map/sample_train_10.csv]\u001b[0m\n",
      "\u001b[34mIndex file location is [/opt/ml/input/data/train/sample_train_index_map/sample_train_10.csv]\u001b[0m\n",
      "\u001b[34mIndex file location is [/opt/ml/input/data/train/sample_train_index_map/sample_train_10.csv]\u001b[0m\n",
      "\u001b[34mIndex file location is [/opt/ml/input/data/train/sample_train_index_map/sample_train_10.csv]\u001b[0m\n",
      "\u001b[34mIndex file folder is [/opt/ml/input/data/train/sample_train_index_map]\u001b[0m\n",
      "\u001b[34mIndex file location is [/opt/ml/input/data/train/sample_train_index_map/sample_train_10.csv]\u001b[0m\n",
      "\u001b[35mIndex file folder is [/opt/ml/input/data/train/sample_train_index_map]\u001b[0m\n",
      "\u001b[35mIndex file location is [/opt/ml/input/data/train/sample_train_index_map/sample_train_10.csv]\u001b[0m\n",
      "\u001b[35mIndex file folder is [/opt/ml/input/data/train/sample_train_index_map]\u001b[0m\n",
      "\u001b[35mIndex file folder is [/opt/ml/input/data/train/sample_train_index_map]\u001b[0m\n",
      "\u001b[35mIndex file location is [/opt/ml/input/data/train/sample_train_index_map/sample_train_10.csv]\u001b[0m\n",
      "\u001b[35mIndex file location is [/opt/ml/input/data/train/sample_train_index_map/sample_train_10.csv]\u001b[0m\n",
      "\u001b[35mIndex file folder is [/opt/ml/input/data/train/sample_train_index_map]\u001b[0m\n",
      "\u001b[35mIndex file location is [/opt/ml/input/data/train/sample_train_index_map/sample_train_10.csv]\u001b[0m\n",
      "\u001b[35mIndex file folder is [/opt/ml/input/data/train/sample_train_index_map]\u001b[0m\n",
      "\u001b[35mIndex file location is [/opt/ml/input/data/train/sample_train_index_map/sample_train_10.csv]\u001b[0m\n",
      "\u001b[35mIndex file folder is [/opt/ml/input/data/train/sample_train_index_map]\u001b[0m\n",
      "\u001b[35mIndex file location is [/opt/ml/input/data/train/sample_train_index_map/sample_train_10.csv]\u001b[0m\n",
      "\u001b[35mIndex file folder is [/opt/ml/input/data/train/sample_train_index_map]\u001b[0m\n",
      "\u001b[35mIndex file folder is [/opt/ml/input/data/train/sample_train_index_map]\u001b[0m\n",
      "\u001b[35mIndex file location is [/opt/ml/input/data/train/sample_train_index_map/sample_train_10.csv]\u001b[0m\n",
      "\u001b[35mIndex file location is [/opt/ml/input/data/train/sample_train_index_map/sample_train_10.csv]\u001b[0m\n",
      "\u001b[35mIndex file folder is [/opt/ml/input/data/test/sample_test_index_map]\u001b[0m\n",
      "\u001b[35mIndex file folder is [/opt/ml/input/data/test/sample_test_index_map]Index file folder is [/opt/ml/input/data/test/sample_test_index_map]\u001b[0m\n",
      "\u001b[35mIndex file folder is [/opt/ml/input/data/test/sample_test_index_map]\u001b[0m\n",
      "\u001b[35mIndex file folder is [/opt/ml/input/data/test/sample_test_index_map]\u001b[0m\n",
      "\u001b[35mIndex file folder is [/opt/ml/input/data/test/sample_test_index_map]\u001b[0m\n",
      "\u001b[35mIndex file location is [/opt/ml/input/data/test/sample_test_index_map/sample_test_10.csv]\u001b[0m\n",
      "\u001b[35mIndex file folder is [/opt/ml/input/data/test/sample_test_index_map]\u001b[0m\n",
      "\u001b[35mIndex file location is [/opt/ml/input/data/test/sample_test_index_map/sample_test_10.csv]\u001b[0m\n",
      "\u001b[35mIndex file location is [/opt/ml/input/data/test/sample_test_index_map/sample_test_10.csv]\u001b[0m\n",
      "\u001b[35mIndex file location is [/opt/ml/input/data/test/sample_test_index_map/sample_test_10.csv]\u001b[0m\n",
      "\u001b[35mIndex file folder is [/opt/ml/input/data/test/sample_test_index_map]\u001b[0m\n",
      "\u001b[35mIndex file location is [/opt/ml/input/data/test/sample_test_index_map/sample_test_10.csv]\u001b[0m\n",
      "\u001b[35mIndex file location is [/opt/ml/input/data/test/sample_test_index_map/sample_test_10.csv]\u001b[0m\n",
      "\u001b[35mIndex file location is [/opt/ml/input/data/test/sample_test_index_map/sample_test_10.csv]\u001b[0m\n",
      "\u001b[35mIndex file location is [/opt/ml/input/data/test/sample_test_index_map/sample_test_10.csv]\u001b[0m\n",
      "\u001b[34mIndex file folder is [/opt/ml/input/data/test/sample_test_index_map]Index file folder is [/opt/ml/input/data/test/sample_test_index_map]\u001b[0m\n",
      "\u001b[34mIndex file folder is [/opt/ml/input/data/test/sample_test_index_map]Index file folder is [/opt/ml/input/data/test/sample_test_index_map]Index file folder is [/opt/ml/input/data/test/sample_test_index_map]\u001b[0m\n",
      "\u001b[34mIndex file folder is [/opt/ml/input/data/test/sample_test_index_map]\u001b[0m\n",
      "\u001b[34mIndex file folder is [/opt/ml/input/data/test/sample_test_index_map]\u001b[0m\n",
      "\u001b[34mIndex file folder is [/opt/ml/input/data/test/sample_test_index_map]\u001b[0m\n",
      "\u001b[34mIndex file location is [/opt/ml/input/data/test/sample_test_index_map/sample_test_10.csv]\u001b[0m\n",
      "\u001b[34mIndex file location is [/opt/ml/input/data/test/sample_test_index_map/sample_test_10.csv]\u001b[0m\n",
      "\u001b[34mIndex file location is [/opt/ml/input/data/test/sample_test_index_map/sample_test_10.csv]\u001b[0m\n",
      "\u001b[34mIndex file location is [/opt/ml/input/data/test/sample_test_index_map/sample_test_10.csv]\u001b[0m\n",
      "\u001b[34mIndex file location is [/opt/ml/input/data/test/sample_test_index_map/sample_test_10.csv]\u001b[0m\n",
      "\u001b[34mIndex file location is [/opt/ml/input/data/test/sample_test_index_map/sample_test_10.csv]\u001b[0m\n",
      "\u001b[34mIndex file location is [/opt/ml/input/data/test/sample_test_index_map/sample_test_10.csv]\u001b[0m\n",
      "\u001b[34mIndex file location is [/opt/ml/input/data/test/sample_test_index_map/sample_test_10.csv]\u001b[0m\n",
      "\u001b[34mLocal Rank is : 0\u001b[0m\n",
      "\u001b[34mWorldsize is : 16\u001b[0m\n",
      "\u001b[34mRank is : 0\u001b[0m\n",
      "\u001b[34mMaster address is : algo-1\u001b[0m\n",
      "\u001b[34mMaster port is : 7777\u001b[0m\n",
      "\u001b[34m(…)UR50D/resolve/main/tokenizer_config.json:   0%|          | 0.00/95.0 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m(…)UR50D/resolve/main/tokenizer_config.json: 100%|██████████| 95.0/95.0 [00:00<00:00, 14.3kB/s]\u001b[0m\n",
      "\u001b[34m(…)m2_t33_650M_UR50D/resolve/main/vocab.txt:   0%|          | 0.00/93.0 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m(…)m2_t33_650M_UR50D/resolve/main/vocab.txt: 100%|██████████| 93.0/93.0 [00:00<00:00, 18.6kB/s]\u001b[0m\n",
      "\u001b[34m(…)50D/resolve/main/special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m(…)50D/resolve/main/special_tokens_map.json: 100%|██████████| 125/125 [00:00<00:00, 201kB/s]\u001b[0m\n",
      "\u001b[34m(…)_t33_650M_UR50D/resolve/main/config.json:   0%|          | 0.00/724 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m(…)_t33_650M_UR50D/resolve/main/config.json: 100%|██████████| 724/724 [00:00<00:00, 1.29MB/s]\u001b[0m\n",
      "\u001b[34mmodel.safetensors:   0%|          | 0.00/2.61G [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mmodel.safetensors:   1%|          | 31.5M/2.61G [00:00<00:08, 304MB/s]\u001b[0m\n",
      "\u001b[34mmodel.safetensors:   3%|▎         | 73.4M/2.61G [00:00<00:07, 360MB/s]\u001b[0m\n",
      "\u001b[34mmodel.safetensors:   4%|▍         | 115M/2.61G [00:00<00:07, 349MB/s]\u001b[0m\n",
      "\u001b[34mmodel.safetensors:   6%|▌         | 157M/2.61G [00:00<00:06, 369MB/s]\u001b[0m\n",
      "\u001b[34mmodel.safetensors:   8%|▊         | 199M/2.61G [00:00<00:06, 382MB/s]\u001b[0m\n",
      "\u001b[35mLocal Rank is : 0\u001b[0m\n",
      "\u001b[35mWorldsize is : 16\u001b[0m\n",
      "\u001b[35mRank is : 8\u001b[0m\n",
      "\u001b[35mMaster address is : algo-1\u001b[0m\n",
      "\u001b[35mMaster port is : 7777\u001b[0m\n",
      "\u001b[35m(…)UR50D/resolve/main/tokenizer_config.json:   0%|          | 0.00/95.0 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[35m(…)UR50D/resolve/main/tokenizer_config.json: 100%|██████████| 95.0/95.0 [00:00<00:00, 14.7kB/s]\u001b[0m\n",
      "\u001b[35m(…)m2_t33_650M_UR50D/resolve/main/vocab.txt:   0%|          | 0.00/93.0 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[35m(…)m2_t33_650M_UR50D/resolve/main/vocab.txt: 100%|██████████| 93.0/93.0 [00:00<00:00, 17.8kB/s]\u001b[0m\n",
      "\u001b[35m(…)50D/resolve/main/special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[35m(…)50D/resolve/main/special_tokens_map.json: 100%|██████████| 125/125 [00:00<00:00, 19.8kB/s]\u001b[0m\n",
      "\u001b[35m(…)_t33_650M_UR50D/resolve/main/config.json:   0%|          | 0.00/724 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[35m(…)_t33_650M_UR50D/resolve/main/config.json: 100%|██████████| 724/724 [00:00<00:00, 134kB/s]\u001b[0m\n",
      "\u001b[35mmodel.safetensors:   0%|          | 0.00/2.61G [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[35mmodel.safetensors:   1%|          | 31.5M/2.61G [00:00<00:08, 296MB/s]\u001b[0m\n",
      "\u001b[35mmodel.safetensors:   3%|▎         | 83.9M/2.61G [00:00<00:06, 400MB/s]\u001b[0m\n",
      "\u001b[35mmodel.safetensors:   5%|▌         | 136M/2.61G [00:00<00:05, 430MB/s]\u001b[0m\n",
      "\u001b[35mmodel.safetensors:   7%|▋         | 189M/2.61G [00:00<00:05, 441MB/s]\u001b[0m\n",
      "\u001b[35mmodel.safetensors:   9%|▉         | 241M/2.61G [00:00<00:05, 449MB/s]\u001b[0m\n",
      "\u001b[35mmodel.safetensors:  11%|█▏        | 294M/2.61G [00:00<00:05, 457MB/s]\u001b[0m\n",
      "\u001b[34mmodel.safetensors:   9%|▉         | 241M/2.61G [00:00<00:06, 386MB/s]\u001b[0m\n",
      "\u001b[34mmodel.safetensors:  11%|█         | 283M/2.61G [00:00<00:05, 391MB/s]\u001b[0m\n",
      "\u001b[34mmodel.safetensors:  12%|█▏        | 325M/2.61G [00:00<00:05, 394MB/s]\u001b[0m\n",
      "\u001b[34mmodel.safetensors:  14%|█▍        | 367M/2.61G [00:00<00:05, 377MB/s]\u001b[0m\n",
      "\u001b[34mmodel.safetensors:  16%|█▌        | 409M/2.61G [00:01<00:05, 378MB/s]\u001b[0m\n",
      "\u001b[34mmodel.safetensors:  17%|█▋        | 451M/2.61G [00:01<00:05, 361MB/s]\u001b[0m\n",
      "\u001b[34mmodel.safetensors:  19%|█▉        | 493M/2.61G [00:01<00:05, 356MB/s]\u001b[0m\n",
      "\u001b[34mmodel.safetensors:  20%|██        | 535M/2.61G [00:01<00:06, 344MB/s]\u001b[0m\n",
      "\u001b[35mmodel.safetensors:  13%|█▎        | 346M/2.61G [00:00<00:04, 465MB/s]\u001b[0m\n",
      "\u001b[35mmodel.safetensors:  15%|█▌        | 398M/2.61G [00:00<00:04, 467MB/s]\u001b[0m\n",
      "\u001b[35mmodel.safetensors:  17%|█▋        | 451M/2.61G [00:01<00:04, 468MB/s]\u001b[0m\n",
      "\u001b[35mmodel.safetensors:  19%|█▉        | 503M/2.61G [00:01<00:04, 471MB/s]\u001b[0m\n",
      "\u001b[35mmodel.safetensors:  21%|██▏       | 556M/2.61G [00:01<00:04, 477MB/s]\u001b[0m\n",
      "\u001b[35mmodel.safetensors:  23%|██▎       | 608M/2.61G [00:01<00:04, 488MB/s]\u001b[0m\n",
      "\u001b[35mmodel.safetensors:  25%|██▌       | 661M/2.61G [00:01<00:03, 496MB/s]\u001b[0m\n",
      "\u001b[35mmodel.safetensors:  27%|██▋       | 713M/2.61G [00:01<00:03, 501MB/s]\u001b[0m\n",
      "\u001b[35mmodel.safetensors:  29%|██▉       | 765M/2.61G [00:01<00:03, 470MB/s]\u001b[0m\n",
      "\u001b[34mmodel.safetensors:  23%|██▎       | 587M/2.61G [00:01<00:05, 368MB/s]\u001b[0m\n",
      "\u001b[34mmodel.safetensors:  24%|██▍       | 629M/2.61G [00:01<00:05, 381MB/s]\u001b[0m\n",
      "\u001b[34mmodel.safetensors:  26%|██▌       | 682M/2.61G [00:01<00:04, 396MB/s]\u001b[0m\n",
      "\u001b[34mmodel.safetensors:  28%|██▊       | 724M/2.61G [00:01<00:04, 379MB/s]\u001b[0m\n",
      "\u001b[34mmodel.safetensors:  29%|██▉       | 765M/2.61G [00:02<00:04, 384MB/s]\u001b[0m\n",
      "\u001b[34mmodel.safetensors:  31%|███       | 807M/2.61G [00:02<00:04, 385MB/s]\u001b[0m\n",
      "\u001b[34mmodel.safetensors:  33%|███▎      | 860M/2.61G [00:02<00:04, 401MB/s]\u001b[0m\n",
      "\u001b[34mmodel.safetensors:  35%|███▍      | 912M/2.61G [00:02<00:04, 410MB/s]\u001b[0m\n",
      "\u001b[34mmodel.safetensors:  37%|███▋      | 965M/2.61G [00:02<00:03, 414MB/s]\u001b[0m\n",
      "\u001b[35mmodel.safetensors:  31%|███▏      | 818M/2.61G [00:01<00:03, 473MB/s]\u001b[0m\n",
      "\u001b[35mmodel.safetensors:  33%|███▎      | 870M/2.61G [00:01<00:03, 460MB/s]\u001b[0m\n",
      "\u001b[35mmodel.safetensors:  35%|███▌      | 923M/2.61G [00:01<00:03, 471MB/s]\u001b[0m\n",
      "\u001b[35mmodel.safetensors:  37%|███▋      | 975M/2.61G [00:02<00:03, 471MB/s]\u001b[0m\n",
      "\u001b[35mmodel.safetensors:  39%|███▉      | 1.03G/2.61G [00:02<00:03, 452MB/s]\u001b[0m\n",
      "\u001b[35mmodel.safetensors:  41%|████▏     | 1.08G/2.61G [00:02<00:03, 464MB/s]\u001b[0m\n",
      "\u001b[35mmodel.safetensors:  43%|████▎     | 1.13G/2.61G [00:02<00:03, 454MB/s]\u001b[0m\n",
      "\u001b[35mmodel.safetensors:  45%|████▌     | 1.18G/2.61G [00:02<00:03, 470MB/s]\u001b[0m\n",
      "\u001b[35mmodel.safetensors:  47%|████▋     | 1.24G/2.61G [00:02<00:02, 474MB/s]\u001b[0m\n",
      "\u001b[34mmodel.safetensors:  39%|███▉      | 1.02G/2.61G [00:02<00:03, 416MB/s]\u001b[0m\n",
      "\u001b[34mmodel.safetensors:  41%|████      | 1.06G/2.61G [00:02<00:03, 416MB/s]\u001b[0m\n",
      "\u001b[34mmodel.safetensors:  42%|████▏     | 1.10G/2.61G [00:02<00:03, 414MB/s]\u001b[0m\n",
      "\u001b[34mmodel.safetensors:  44%|████▍     | 1.15G/2.61G [00:02<00:03, 417MB/s]\u001b[0m\n",
      "\u001b[34mmodel.safetensors:  46%|████▌     | 1.21G/2.61G [00:03<00:03, 418MB/s]\u001b[0m\n",
      "\u001b[34mmodel.safetensors:  48%|████▊     | 1.26G/2.61G [00:03<00:03, 408MB/s]\u001b[0m\n",
      "\u001b[34mmodel.safetensors:  50%|█████     | 1.31G/2.61G [00:03<00:03, 415MB/s]\u001b[0m\n",
      "\u001b[34mmodel.safetensors:  52%|█████▏    | 1.35G/2.61G [00:03<00:03, 394MB/s]\u001b[0m\n",
      "\u001b[35mmodel.safetensors:  49%|████▉     | 1.29G/2.61G [00:02<00:02, 479MB/s]\u001b[0m\n",
      "\u001b[35mmodel.safetensors:  51%|█████▏    | 1.34G/2.61G [00:02<00:02, 487MB/s]\u001b[0m\n",
      "\u001b[35mmodel.safetensors:  53%|█████▎    | 1.39G/2.61G [00:02<00:02, 490MB/s]\u001b[0m\n",
      "\u001b[35mmodel.safetensors:  55%|█████▌    | 1.45G/2.61G [00:03<00:02, 486MB/s]\u001b[0m\n",
      "\u001b[35mmodel.safetensors:  57%|█████▋    | 1.50G/2.61G [00:03<00:02, 487MB/s]\u001b[0m\n",
      "\u001b[35mmodel.safetensors:  59%|█████▉    | 1.55G/2.61G [00:03<00:02, 490MB/s]\u001b[0m\n",
      "\u001b[35mmodel.safetensors:  61%|██████▏   | 1.60G/2.61G [00:03<00:02, 490MB/s]\u001b[0m\n",
      "\u001b[35mmodel.safetensors:  63%|██████▎   | 1.66G/2.61G [00:03<00:01, 499MB/s]\u001b[0m\n",
      "\u001b[35mmodel.safetensors:  65%|██████▌   | 1.71G/2.61G [00:03<00:01, 501MB/s]\u001b[0m\n",
      "\u001b[34mmodel.safetensors:  54%|█████▍    | 1.41G/2.61G [00:03<00:02, 402MB/s]\u001b[0m\n",
      "\u001b[34mmodel.safetensors:  55%|█████▌    | 1.45G/2.61G [00:03<00:02, 406MB/s]\u001b[0m\n",
      "\u001b[34mmodel.safetensors:  57%|█████▋    | 1.49G/2.61G [00:03<00:02, 407MB/s]\u001b[0m\n",
      "\u001b[34mmodel.safetensors:  59%|█████▉    | 1.54G/2.61G [00:03<00:02, 419MB/s]\u001b[0m\n",
      "\u001b[34mmodel.safetensors:  61%|██████    | 1.59G/2.61G [00:04<00:02, 424MB/s]\u001b[0m\n",
      "\u001b[34mmodel.safetensors:  63%|██████▎   | 1.65G/2.61G [00:04<00:02, 414MB/s]\u001b[0m\n",
      "\u001b[34mmodel.safetensors:  65%|██████▌   | 1.70G/2.61G [00:04<00:02, 410MB/s]\u001b[0m\n",
      "\u001b[34mmodel.safetensors:  67%|██████▋   | 1.75G/2.61G [00:04<00:02, 414MB/s]\u001b[0m\n",
      "\u001b[35mmodel.safetensors:  68%|██████▊   | 1.76G/2.61G [00:03<00:01, 485MB/s]\u001b[0m\n",
      "\u001b[35mmodel.safetensors:  70%|██████▉   | 1.81G/2.61G [00:03<00:01, 473MB/s]\u001b[0m\n",
      "\u001b[35mmodel.safetensors:  72%|███████▏  | 1.87G/2.61G [00:03<00:01, 461MB/s]\u001b[0m\n",
      "\u001b[35mmodel.safetensors:  74%|███████▎  | 1.92G/2.61G [00:04<00:01, 474MB/s]\u001b[0m\n",
      "\u001b[35mmodel.safetensors:  76%|███████▌  | 1.97G/2.61G [00:04<00:01, 472MB/s]\u001b[0m\n",
      "\u001b[35mmodel.safetensors:  78%|███████▊  | 2.02G/2.61G [00:04<00:01, 480MB/s]\u001b[0m\n",
      "\u001b[35mmodel.safetensors:  80%|███████▉  | 2.08G/2.61G [00:04<00:01, 487MB/s]\u001b[0m\n",
      "\u001b[35mmodel.safetensors:  82%|████████▏ | 2.13G/2.61G [00:04<00:00, 489MB/s]\u001b[0m\n",
      "\u001b[35mmodel.safetensors:  84%|████████▎ | 2.18G/2.61G [00:04<00:00, 469MB/s]\u001b[0m\n",
      "\u001b[34mmodel.safetensors:  69%|██████▉   | 1.80G/2.61G [00:04<00:01, 417MB/s]\u001b[0m\n",
      "\u001b[34mmodel.safetensors:  71%|███████   | 1.86G/2.61G [00:04<00:01, 419MB/s]\u001b[0m\n",
      "\u001b[34mmodel.safetensors:  73%|███████▎  | 1.91G/2.61G [00:04<00:01, 420MB/s]\u001b[0m\n",
      "\u001b[34mmodel.safetensors:  75%|███████▌  | 1.96G/2.61G [00:04<00:01, 424MB/s]\u001b[0m\n",
      "\u001b[34mmodel.safetensors:  77%|███████▋  | 2.01G/2.61G [00:05<00:01, 428MB/s]\u001b[0m\n",
      "\u001b[34mmodel.safetensors:  79%|███████▉  | 2.07G/2.61G [00:05<00:01, 427MB/s]\u001b[0m\n",
      "\u001b[34mmodel.safetensors:  81%|████████  | 2.12G/2.61G [00:05<00:01, 428MB/s]\u001b[0m\n",
      "\u001b[34mmodel.safetensors:  83%|████████▎ | 2.17G/2.61G [00:05<00:01, 431MB/s]\u001b[0m\n",
      "\u001b[34mmodel.safetensors:  85%|████████▌ | 2.22G/2.61G [00:05<00:00, 432MB/s]\u001b[0m\n",
      "\u001b[35mmodel.safetensors:  86%|████████▌ | 2.23G/2.61G [00:04<00:00, 470MB/s]\u001b[0m\n",
      "\u001b[35mmodel.safetensors:  88%|████████▊ | 2.29G/2.61G [00:04<00:00, 472MB/s]\u001b[0m\n",
      "\u001b[35mmodel.safetensors:  90%|████████▉ | 2.34G/2.61G [00:04<00:00, 483MB/s]\u001b[0m\n",
      "\u001b[35mmodel.safetensors:  92%|█████████▏| 2.39G/2.61G [00:05<00:00, 479MB/s]\u001b[0m\n",
      "\u001b[35mmodel.safetensors:  94%|█████████▎| 2.44G/2.61G [00:05<00:00, 458MB/s]\u001b[0m\n",
      "\u001b[35mmodel.safetensors:  96%|█████████▌| 2.50G/2.61G [00:05<00:00, 471MB/s]\u001b[0m\n",
      "\u001b[35mmodel.safetensors:  98%|█████████▊| 2.55G/2.61G [00:05<00:00, 483MB/s]\u001b[0m\n",
      "\u001b[35mmodel.safetensors: 100%|█████████▉| 2.60G/2.61G [00:05<00:00, 492MB/s]\u001b[0m\n",
      "\u001b[35mmodel.safetensors: 100%|██████████| 2.61G/2.61G [00:05<00:00, 474MB/s]\u001b[0m\n",
      "\u001b[34mmodel.safetensors:  87%|████████▋ | 2.28G/2.61G [00:05<00:00, 435MB/s]\u001b[0m\n",
      "\u001b[34mmodel.safetensors:  89%|████████▉ | 2.33G/2.61G [00:05<00:00, 437MB/s]\u001b[0m\n",
      "\u001b[34mmodel.safetensors:  91%|█████████ | 2.38G/2.61G [00:05<00:00, 437MB/s]\u001b[0m\n",
      "\u001b[34mmodel.safetensors:  93%|█████████▎| 2.43G/2.61G [00:06<00:00, 436MB/s]\u001b[0m\n",
      "\u001b[34mmodel.safetensors:  95%|█████████▌| 2.49G/2.61G [00:06<00:00, 426MB/s]\u001b[0m\n",
      "\u001b[34mmodel.safetensors:  97%|█████████▋| 2.54G/2.61G [00:06<00:00, 424MB/s]\u001b[0m\n",
      "\u001b[34mmodel.safetensors:  99%|█████████▉| 2.59G/2.61G [00:06<00:00, 414MB/s]\u001b[0m\n",
      "\u001b[34mmodel.safetensors: 100%|██████████| 2.61G/2.61G [00:06<00:00, 405MB/s]\u001b[0m\n",
      "\u001b[35m[2023-10-13 04:43:28.852 algo-2:207 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[35m[2023-10-13 04:43:28.906 algo-2:207 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[35m[2023-10-13 04:43:28.906 algo-2:207 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[35m[2023-10-13 04:43:28.907 algo-2:207 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[35m[2023-10-13 04:43:28.907 algo-2:207 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[35m[2023-10-13 04:43:28.907 algo-2:207 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[35m[2023-10-13 04:43:29.393 algo-2:202 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[35m[2023-10-13 04:43:29.413 algo-2:204 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[35m[2023-10-13 04:43:29.419 algo-2:201 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[35m[2023-10-13 04:43:29.448 algo-2:202 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[35m[2023-10-13 04:43:29.448 algo-2:202 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[35m[2023-10-13 04:43:29.449 algo-2:202 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[35m[2023-10-13 04:43:29.449 algo-2:202 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[35m[2023-10-13 04:43:29.449 algo-2:202 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[35m[2023-10-13 04:43:29.451 algo-2:205 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[35m[2023-10-13 04:43:29.451 algo-2:203 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[35m[2023-10-13 04:43:29.452 algo-2:208 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[35m[2023-10-13 04:43:29.466 algo-2:204 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[35m[2023-10-13 04:43:29.466 algo-2:204 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[35m[2023-10-13 04:43:29.466 algo-2:204 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[35m[2023-10-13 04:43:29.467 algo-2:204 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[35m[2023-10-13 04:43:29.467 algo-2:204 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[35m[2023-10-13 04:43:29.471 algo-2:201 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[35m[2023-10-13 04:43:29.472 algo-2:201 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[35m[2023-10-13 04:43:29.472 algo-2:201 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[35m[2023-10-13 04:43:29.473 algo-2:201 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[35m[2023-10-13 04:43:29.473 algo-2:201 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[35m[2023-10-13 04:43:29.502 algo-2:206 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[35m[2023-10-13 04:43:29.505 algo-2:203 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[35m[2023-10-13 04:43:29.506 algo-2:205 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[35m[2023-10-13 04:43:29.506 algo-2:203 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[35m[2023-10-13 04:43:29.506 algo-2:203 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[35m[2023-10-13 04:43:29.506 algo-2:205 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[35m[2023-10-13 04:43:29.507 algo-2:205 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[35m[2023-10-13 04:43:29.507 algo-2:203 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[35m[2023-10-13 04:43:29.507 algo-2:203 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[35m[2023-10-13 04:43:29.507 algo-2:208 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[35m[2023-10-13 04:43:29.507 algo-2:205 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[35m[2023-10-13 04:43:29.507 algo-2:205 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[35m[2023-10-13 04:43:29.507 algo-2:208 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[35m[2023-10-13 04:43:29.508 algo-2:208 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[35m[2023-10-13 04:43:29.508 algo-2:208 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[35m[2023-10-13 04:43:29.508 algo-2:208 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[35m[2023-10-13 04:43:29.558 algo-2:206 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[35m[2023-10-13 04:43:29.558 algo-2:206 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[35m[2023-10-13 04:43:29.558 algo-2:206 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[35m[2023-10-13 04:43:29.559 algo-2:206 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[35m[2023-10-13 04:43:29.559 algo-2:206 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2023-10-13 04:43:29.818 algo-1:209 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-10-13 04:43:29.875 algo-1:209 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2023-10-13 04:43:29.876 algo-1:209 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2023-10-13 04:43:29.876 algo-1:209 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2023-10-13 04:43:29.876 algo-1:209 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2023-10-13 04:43:29.877 algo-1:209 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2023-10-13 04:43:30.196 algo-1:208 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m***** Running training *****\u001b[0m\n",
      "\u001b[34mNum examples: 2851861\u001b[0m\n",
      "\u001b[34mNum Epochs: 1\u001b[0m\n",
      "\u001b[34mInstantaneous batch size per device = 12\u001b[0m\n",
      "\u001b[34mTotal train batch size (w. parallel, distributed & accumulation) = 192\u001b[0m\n",
      "\u001b[34mTotal optimization steps = 14854\u001b[0m\n",
      "\u001b[34m0%|          | 0/14854 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m######################### Train #########################\u001b[0m\n",
      "\u001b[34m[2023-10-13 04:43:30.251 algo-1:208 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2023-10-13 04:43:30.252 algo-1:208 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2023-10-13 04:43:30.252 algo-1:208 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2023-10-13 04:43:30.253 algo-1:208 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2023-10-13 04:43:30.253 algo-1:208 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2023-10-13 04:43:30.253 algo-1:206 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-10-13 04:43:30.259 algo-1:202 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-10-13 04:43:30.307 algo-1:206 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2023-10-13 04:43:30.308 algo-1:206 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2023-10-13 04:43:30.308 algo-1:206 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2023-10-13 04:43:30.308 algo-1:206 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2023-10-13 04:43:30.308 algo-1:206 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2023-10-13 04:43:30.310 algo-1:207 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-10-13 04:43:30.312 algo-1:202 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2023-10-13 04:43:30.312 algo-1:202 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2023-10-13 04:43:30.312 algo-1:202 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2023-10-13 04:43:30.313 algo-1:204 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-10-13 04:43:30.313 algo-1:202 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2023-10-13 04:43:30.313 algo-1:202 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2023-10-13 04:43:30.325 algo-1:205 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-10-13 04:43:30.331 algo-1:203 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-10-13 04:43:30.365 algo-1:204 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2023-10-13 04:43:30.366 algo-1:204 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2023-10-13 04:43:30.366 algo-1:207 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2023-10-13 04:43:30.366 algo-1:204 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2023-10-13 04:43:30.367 algo-1:204 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2023-10-13 04:43:30.367 algo-1:207 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2023-10-13 04:43:30.367 algo-1:204 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2023-10-13 04:43:30.367 algo-1:207 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2023-10-13 04:43:30.368 algo-1:207 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2023-10-13 04:43:30.368 algo-1:207 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2023-10-13 04:43:30.381 algo-1:205 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2023-10-13 04:43:30.382 algo-1:205 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2023-10-13 04:43:30.382 algo-1:205 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2023-10-13 04:43:30.383 algo-1:205 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2023-10-13 04:43:30.383 algo-1:205 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2023-10-13 04:43:30.386 algo-1:203 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2023-10-13 04:43:30.386 algo-1:203 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2023-10-13 04:43:30.387 algo-1:203 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2023-10-13 04:43:30.387 algo-1:203 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2023-10-13 04:43:30.387 algo-1:203 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[35m/opt/conda/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[35m/opt/conda/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[35m/opt/conda/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m0%|          | 1/14854 [00:02<8:41:43,  2.11s/it]\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mNCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[35m/opt/conda/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[35m/opt/conda/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[35m/opt/conda/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[35m/opt/conda/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[35m/opt/conda/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"/opt/ml/code/cuda-uniref100-pretorkenized-mlm-train-ddp-fsdp.py\", line 365, in <module>\u001b[0m\n",
      "\u001b[34moutputs = model(**batch)  # Forward pass\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py\", line 2727, in forward\u001b[0m\n",
      "\u001b[34moutput = self._fsdp_wrapped_module(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/flatten_params_wrapper.py\", line 165, in forward\u001b[0m\n",
      "\u001b[34mreturn self.module(*inputs, **kwinputs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 1017, in forward\u001b[0m\n",
      "\u001b[34moutputs = self.esm(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 923, in forward\u001b[0m\n",
      "\u001b[34mencoder_outputs = self.encoder(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 624, in forward\u001b[0m\n",
      "\u001b[34mlayer_outputs = layer_module(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 509, in forward\u001b[0m\n",
      "\u001b[34mself_attention_outputs = self.attention(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 443, in forward\u001b[0m\n",
      "\u001b[34mself_outputs = self.self(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 344, in forward\u001b[0m\n",
      "\u001b[34mquery_layer, key_layer = self.rotary_embeddings(query_layer, key_layer)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 125, in forward\u001b[0m\n",
      "\u001b[34mapply_rotary_pos_emb(q, self._cos_cached, self._sin_cached),\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 60, in apply_rotary_pos_emb\u001b[0m\n",
      "\u001b[34mreturn (x * cos) + (rotate_half(x) * sin)\u001b[0m\n",
      "\u001b[34mtorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 2; 31.74 GiB total capacity; 30.89 GiB already allocated; 15.38 MiB free; 30.90 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"/opt/ml/code/cuda-uniref100-pretorkenized-mlm-train-ddp-fsdp.py\", line 365, in <module>\u001b[0m\n",
      "\u001b[34moutputs = model(**batch)  # Forward pass\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py\", line 2727, in forward\u001b[0m\n",
      "\u001b[34moutput = self._fsdp_wrapped_module(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/flatten_params_wrapper.py\", line 165, in forward\u001b[0m\n",
      "\u001b[34mreturn self.module(*inputs, **kwinputs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 1017, in forward\u001b[0m\n",
      "\u001b[34moutputs = self.esm(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 923, in forward\u001b[0m\n",
      "\u001b[34mencoder_outputs = self.encoder(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 624, in forward\u001b[0m\n",
      "\u001b[34mlayer_outputs = layer_module(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 509, in forward\u001b[0m\n",
      "\u001b[34mself_attention_outputs = self.attention(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 443, in forward\u001b[0m\n",
      "\u001b[34mself_outputs = self.self(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 331, in forward\u001b[0m\n",
      "\u001b[34mquery_layer = query_layer * self.attention_head_size**-0.5\u001b[0m\n",
      "\u001b[34mtorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 3; 31.74 GiB total capacity; 30.86 GiB already allocated; 21.38 MiB free; 30.87 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"/opt/ml/code/cuda-uniref100-pretorkenized-mlm-train-ddp-fsdp.py\", line 365, in <module>\u001b[0m\n",
      "\u001b[34moutputs = model(**batch)  # Forward pass\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py\", line 2727, in forward\u001b[0m\n",
      "\u001b[34moutput = self._fsdp_wrapped_module(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/flatten_params_wrapper.py\", line 165, in forward\u001b[0m\n",
      "\u001b[34mreturn self.module(*inputs, **kwinputs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 1017, in forward\u001b[0m\n",
      "\u001b[34moutputs = self.esm(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 923, in forward\u001b[0m\n",
      "\u001b[34mencoder_outputs = self.encoder(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 624, in forward\u001b[0m\n",
      "\u001b[34mlayer_outputs = layer_module(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 509, in forward\u001b[0m\n",
      "\u001b[34mself_attention_outputs = self.attention(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 443, in forward\u001b[0m\n",
      "\u001b[34mself_outputs = self.self(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 323, in forward\u001b[0m\n",
      "\u001b[34mvalue_layer = self.transpose_for_scores(self.value(hidden_states))\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/linear.py\", line 114, in forward\u001b[0m\n",
      "\u001b[34mreturn F.linear(input, self.weight, self.bias)\u001b[0m\n",
      "\u001b[34mtorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 4; 31.74 GiB total capacity; 30.84 GiB already allocated; 7.38 MiB free; 30.84 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"/opt/ml/code/cuda-uniref100-pretorkenized-mlm-train-ddp-fsdp.py\", line 365, in <module>\u001b[0m\n",
      "\u001b[34moutputs = model(**batch)  # Forward pass\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py\", line 2727, in forward\u001b[0m\n",
      "\u001b[34moutput = self._fsdp_wrapped_module(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/flatten_params_wrapper.py\", line 165, in forward\u001b[0m\n",
      "\u001b[34mreturn self.module(*inputs, **kwinputs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 1017, in forward\u001b[0m\n",
      "\u001b[34moutputs = self.esm(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 923, in forward\u001b[0m\n",
      "\u001b[34mencoder_outputs = self.encoder(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 624, in forward\u001b[0m\n",
      "\u001b[34mlayer_outputs = layer_module(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 509, in forward\u001b[0m\n",
      "\u001b[34mself_attention_outputs = self.attention(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 443, in forward\u001b[0m\n",
      "\u001b[34mself_outputs = self.self(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 323, in forward\u001b[0m\n",
      "\u001b[34mvalue_layer = self.transpose_for_scores(self.value(hidden_states))\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/linear.py\", line 114, in forward\u001b[0m\n",
      "\u001b[34mreturn F.linear(input, self.weight, self.bias)\u001b[0m\n",
      "\u001b[34mtorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 0; 31.74 GiB total capacity; 30.84 GiB already allocated; 27.38 MiB free; 30.84 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"/opt/ml/code/cuda-uniref100-pretorkenized-mlm-train-ddp-fsdp.py\", line 365, in <module>\u001b[0m\n",
      "\u001b[34moutputs = model(**batch)  # Forward pass\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py\", line 2727, in forward\u001b[0m\n",
      "\u001b[34moutput = self._fsdp_wrapped_module(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/flatten_params_wrapper.py\", line 165, in forward\u001b[0m\n",
      "\u001b[34mreturn self.module(*inputs, **kwinputs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 1017, in forward\u001b[0m\n",
      "\u001b[34moutputs = self.esm(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 923, in forward\u001b[0m\n",
      "\u001b[34mencoder_outputs = self.encoder(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 624, in forward\u001b[0m\n",
      "\u001b[34mlayer_outputs = layer_module(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 551, in forward\u001b[0m\n",
      "\u001b[34mlayer_output = self.feed_forward_chunk(attention_output)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 562, in feed_forward_chunk\u001b[0m\n",
      "\u001b[34mintermediate_output = self.intermediate(attention_output_ln)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 464, in forward\u001b[0m\n",
      "\u001b[34mhidden_states = gelu(hidden_states)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 67, in gelu\u001b[0m\n",
      "\u001b[34mreturn x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\u001b[0m\n",
      "\u001b[34mtorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 120.00 MiB (GPU 1; 31.74 GiB total capacity; 30.72 GiB already allocated; 111.38 MiB free; 30.72 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"/opt/ml/code/cuda-uniref100-pretorkenized-mlm-train-ddp-fsdp.py\", line 365, in <module>\u001b[0m\n",
      "\u001b[34moutputs = model(**batch)  # Forward pass\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py\", line 2727, in forward\u001b[0m\n",
      "\u001b[34moutput = self._fsdp_wrapped_module(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/flatten_params_wrapper.py\", line 165, in forward\u001b[0m\n",
      "\u001b[34mreturn self.module(*inputs, **kwinputs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 1017, in forward\u001b[0m\n",
      "\u001b[34moutputs = self.esm(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 923, in forward\u001b[0m\n",
      "\u001b[34mencoder_outputs = self.encoder(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 624, in forward\u001b[0m\n",
      "\u001b[34mlayer_outputs = layer_module(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 509, in forward\u001b[0m\n",
      "\u001b[34mself_attention_outputs = self.attention(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 443, in forward\u001b[0m\n",
      "\u001b[34mself_outputs = self.self(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 331, in forward\u001b[0m\n",
      "\u001b[34mquery_layer = query_layer * self.attention_head_size**-0.5\u001b[0m\n",
      "\u001b[34mtorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 31.74 GiB total capacity; 30.86 GiB already allocated; 5.38 MiB free; 30.87 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"/opt/ml/code/cuda-uniref100-pretorkenized-mlm-train-ddp-fsdp.py\", line 365, in <module>\u001b[0m\n",
      "\u001b[34moutputs = model(**batch)  # Forward pass\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py\", line 2727, in forward\u001b[0m\n",
      "\u001b[34moutput = self._fsdp_wrapped_module(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/flatten_params_wrapper.py\", line 165, in forward\u001b[0m\n",
      "\u001b[34mreturn self.module(*inputs, **kwinputs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 1017, in forward\u001b[0m\n",
      "\u001b[34moutputs = self.esm(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 923, in forward\u001b[0m\n",
      "\u001b[34mencoder_outputs = self.encoder(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 624, in forward\u001b[0m\n",
      "\u001b[34mlayer_outputs = layer_module(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 509, in forward\u001b[0m\n",
      "\u001b[34mself_attention_outputs = self.attention(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 443, in forward\u001b[0m\n",
      "\u001b[34mself_outputs = self.self(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 331, in forward\u001b[0m\n",
      "\u001b[34mquery_layer = query_layer * self.attention_head_size**-0.5\u001b[0m\n",
      "\u001b[34mtorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 7; 31.74 GiB total capacity; 30.86 GiB already allocated; 29.38 MiB free; 30.87 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"/opt/ml/code/cuda-uniref100-pretorkenized-mlm-train-ddp-fsdp.py\", line 365, in <module>\u001b[0m\n",
      "\u001b[34moutputs = model(**batch)  # Forward pass\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py\", line 2727, in forward\u001b[0m\n",
      "\u001b[34moutput = self._fsdp_wrapped_module(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/flatten_params_wrapper.py\", line 165, in forward\u001b[0m\n",
      "\u001b[34mreturn self.module(*inputs, **kwinputs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 1017, in forward\u001b[0m\n",
      "\u001b[34moutputs = self.esm(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 923, in forward\u001b[0m\n",
      "\u001b[34mencoder_outputs = self.encoder(\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 624, in forward\u001b[0m\n",
      "\u001b[34mlayer_outputs = layer_module(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 509, in forward\u001b[0m\n",
      "\u001b[34mself_attention_outputs = self.attention(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 443, in forward\u001b[0m\n",
      "\u001b[34mself_outputs = self.self(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 331, in forward\u001b[0m\n",
      "\u001b[34mquery_layer = query_layer * self.attention_head_size**-0.5\u001b[0m\n",
      "\u001b[34mtorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 5; 31.74 GiB total capacity; 30.86 GiB already allocated; 21.38 MiB free; 30.87 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\u001b[0m\n",
      "\u001b[35mTraceback (most recent call last):\n",
      "  File \"/opt/ml/code/cuda-uniref100-pretorkenized-mlm-train-ddp-fsdp.py\", line 365, in <module>\u001b[0m\n",
      "\u001b[35moutputs = model(**batch)  # Forward pass\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[35mreturn forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[35mFile \"/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py\", line 2727, in forward\u001b[0m\n",
      "\u001b[35moutput = self._fsdp_wrapped_module(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[35mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/flatten_params_wrapper.py\", line 165, in forward\u001b[0m\n",
      "\u001b[35mreturn self.module(*inputs, **kwinputs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[35mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 1017, in forward\u001b[0m\n",
      "\u001b[35moutputs = self.esm(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[35mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 923, in forward\u001b[0m\n",
      "\u001b[35mencoder_outputs = self.encoder(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[35mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 624, in forward\u001b[0m\n",
      "\u001b[35mlayer_outputs = layer_module(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[35mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 509, in forward\u001b[0m\n",
      "\u001b[35mself_attention_outputs = self.attention(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[35mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 443, in forward\u001b[0m\n",
      "\u001b[35mself_outputs = self.self(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[35mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 344, in forward\u001b[0m\n",
      "\u001b[35mquery_layer, key_layer = self.rotary_embeddings(query_layer, key_layer)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[35mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 125, in forward\u001b[0m\n",
      "\u001b[35mapply_rotary_pos_emb(q, self._cos_cached, self._sin_cached),\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 60, in apply_rotary_pos_emb\u001b[0m\n",
      "\u001b[35mreturn (x * cos) + (rotate_half(x) * sin)\u001b[0m\n",
      "\u001b[35mtorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 2; 31.74 GiB total capacity; 30.89 GiB already allocated; 15.38 MiB free; 30.90 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\u001b[0m\n",
      "\u001b[35mTraceback (most recent call last):\n",
      "  File \"/opt/ml/code/cuda-uniref100-pretorkenized-mlm-train-ddp-fsdp.py\", line 365, in <module>\u001b[0m\n",
      "\u001b[35moutputs = model(**batch)  # Forward pass\u001b[0m\n",
      "\u001b[35mFile \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[35mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py\", line 2727, in forward\u001b[0m\n",
      "\u001b[35moutput = self._fsdp_wrapped_module(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[35mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/flatten_params_wrapper.py\", line 165, in forward\u001b[0m\n",
      "\u001b[35mreturn self.module(*inputs, **kwinputs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[35mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 1017, in forward\u001b[0m\n",
      "\u001b[35moutputs = self.esm(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[35mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 923, in forward\u001b[0m\n",
      "\u001b[35mencoder_outputs = self.encoder(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[35mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 624, in forward\u001b[0m\n",
      "\u001b[35mlayer_outputs = layer_module(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[35mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 509, in forward\u001b[0m\n",
      "\u001b[35mself_attention_outputs = self.attention(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[35mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 443, in forward\u001b[0m\n",
      "\u001b[35mself_outputs = self.self(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[35mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 323, in forward\u001b[0m\n",
      "\u001b[35mvalue_layer = self.transpose_for_scores(self.value(hidden_states))\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[35mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/linear.py\", line 114, in forward\u001b[0m\n",
      "\u001b[35mreturn F.linear(input, self.weight, self.bias)\u001b[0m\n",
      "\u001b[35mtorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 4; 31.74 GiB total capacity; 30.84 GiB already allocated; 7.38 MiB free; 30.84 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\u001b[0m\n",
      "\u001b[35mTraceback (most recent call last):\n",
      "  File \"/opt/ml/code/cuda-uniref100-pretorkenized-mlm-train-ddp-fsdp.py\", line 365, in <module>\u001b[0m\n",
      "\u001b[35moutputs = model(**batch)  # Forward pass\u001b[0m\n",
      "\u001b[35mFile \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[35mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py\", line 2727, in forward\u001b[0m\n",
      "\u001b[35moutput = self._fsdp_wrapped_module(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[35mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/flatten_params_wrapper.py\", line 165, in forward\u001b[0m\n",
      "\u001b[35mreturn self.module(*inputs, **kwinputs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[35mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 1017, in forward\u001b[0m\n",
      "\u001b[35moutputs = self.esm(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[35mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 923, in forward\u001b[0m\n",
      "\u001b[35mencoder_outputs = self.encoder(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[35mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 624, in forward\u001b[0m\n",
      "\u001b[35mlayer_outputs = layer_module(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[35mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 509, in forward\u001b[0m\n",
      "\u001b[35mself_attention_outputs = self.attention(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[35mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 443, in forward\u001b[0m\n",
      "\u001b[35mself_outputs = self.self(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[35mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 331, in forward\u001b[0m\n",
      "\u001b[35mquery_layer = query_layer * self.attention_head_size**-0.5\u001b[0m\n",
      "\u001b[35mtorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 5; 31.74 GiB total capacity; 30.86 GiB already allocated; 21.38 MiB free; 30.87 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\u001b[0m\n",
      "\u001b[35mTraceback (most recent call last):\n",
      "  File \"/opt/ml/code/cuda-uniref100-pretorkenized-mlm-train-ddp-fsdp.py\", line 365, in <module>\u001b[0m\n",
      "\u001b[35moutputs = model(**batch)  # Forward pass\u001b[0m\n",
      "\u001b[35mFile \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[35mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py\", line 2727, in forward\u001b[0m\n",
      "\u001b[35moutput = self._fsdp_wrapped_module(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[35mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/flatten_params_wrapper.py\", line 165, in forward\u001b[0m\n",
      "\u001b[35mreturn self.module(*inputs, **kwinputs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[35mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 1017, in forward\u001b[0m\n",
      "\u001b[35moutputs = self.esm(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[35mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 923, in forward\u001b[0m\n",
      "\u001b[35mencoder_outputs = self.encoder(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[35mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 624, in forward\u001b[0m\n",
      "\u001b[35mlayer_outputs = layer_module(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[35mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 551, in forward\u001b[0m\n",
      "\u001b[35mlayer_output = self.feed_forward_chunk(attention_output)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 562, in feed_forward_chunk\u001b[0m\n",
      "\u001b[35mintermediate_output = self.intermediate(attention_output_ln)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[35mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 464, in forward\u001b[0m\n",
      "\u001b[35mhidden_states = gelu(hidden_states)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 67, in gelu\u001b[0m\n",
      "\u001b[35mreturn x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\u001b[0m\n",
      "\u001b[35mtorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 120.00 MiB (GPU 1; 31.74 GiB total capacity; 30.72 GiB already allocated; 111.38 MiB free; 30.72 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\u001b[0m\n",
      "\u001b[35mTraceback (most recent call last):\u001b[0m\n",
      "\u001b[35mTraceback (most recent call last):\n",
      "  File \"/opt/ml/code/cuda-uniref100-pretorkenized-mlm-train-ddp-fsdp.py\", line 365, in <module>\n",
      "  File \"/opt/ml/code/cuda-uniref100-pretorkenized-mlm-train-ddp-fsdp.py\", line 365, in <module>\u001b[0m\n",
      "\u001b[35moutputs = model(**batch)  # Forward passoutputs = model(**batch)  # Forward pass\u001b[0m\n",
      "\u001b[35mFile \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[35mreturn forward_call(*input, **kwargs)return forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py\", line 2727, in forward\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py\", line 2727, in forward\u001b[0m\n",
      "\u001b[35moutput = self._fsdp_wrapped_module(*args, **kwargs)output = self._fsdp_wrapped_module(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[35mreturn forward_call(*input, **kwargs)return forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/flatten_params_wrapper.py\", line 165, in forward\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/flatten_params_wrapper.py\", line 165, in forward\u001b[0m\n",
      "\u001b[35mreturn self.module(*inputs, **kwinputs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[35mreturn self.module(*inputs, **kwinputs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[35mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 1017, in forward\u001b[0m\n",
      "\u001b[35mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 1017, in forward\u001b[0m\n",
      "\u001b[35moutputs = self.esm(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    outputs = self.esm(\u001b[0m\n",
      "\u001b[35mFile \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[35mreturn forward_call(*input, **kwargs)\n",
      "    return forward_call(*input, **kwargs)  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 923, in forward\u001b[0m\n",
      "\u001b[35mFile \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 923, in forward\u001b[0m\n",
      "\u001b[35mencoder_outputs = self.encoder(encoder_outputs = self.encoder(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[35mreturn forward_call(*input, **kwargs)return forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 624, in forward\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 624, in forward\u001b[0m\n",
      "\u001b[35mlayer_outputs = layer_module(layer_outputs = layer_module(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[35mFile \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[35mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 509, in forward\u001b[0m\n",
      "\u001b[35mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 509, in forward\u001b[0m\n",
      "\u001b[35mself_attention_outputs = self.attention(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[35mself_attention_outputs = self.attention(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[35mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 443, in forward\u001b[0m\n",
      "\u001b[35mself_outputs = self.self(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[35mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 443, in forward\u001b[0m\n",
      "\u001b[35mself_outputs = self.self(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[35mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 331, in forward\u001b[0m\n",
      "\u001b[35mTraceback (most recent call last):\n",
      "  File \"/opt/ml/code/cuda-uniref100-pretorkenized-mlm-train-ddp-fsdp.py\", line 365, in <module>\u001b[0m\n",
      "\u001b[35mquery_layer = query_layer * self.attention_head_size**-0.5\u001b[0m\n",
      "\u001b[35mtorch.cuda.OutOfMemoryError    : return forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[35mCUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 31.74 GiB total capacity; 30.86 GiB already allocated; 5.38 MiB free; 30.87 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 331, in forward\u001b[0m\n",
      "\u001b[35mquery_layer = query_layer * self.attention_head_size**-0.5\u001b[0m\n",
      "\u001b[35mtorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 7; 31.74 GiB total capacity; 30.86 GiB already allocated; 23.38 MiB free; 30.87 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "    outputs = model(**batch)  # Forward pass\u001b[0m\n",
      "\u001b[35mFile \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[35mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py\", line 2727, in forward\u001b[0m\n",
      "\u001b[35moutput = self._fsdp_wrapped_module(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[35mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/flatten_params_wrapper.py\", line 165, in forward\u001b[0m\n",
      "\u001b[35mreturn self.module(*inputs, **kwinputs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[35mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 1017, in forward\u001b[0m\n",
      "\u001b[35moutputs = self.esm(\u001b[0m\n",
      "\u001b[35mFile \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[35mreturn forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[35mFile \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 923, in forward\u001b[0m\n",
      "\u001b[35mencoder_outputs = self.encoder(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[35mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 624, in forward\u001b[0m\n",
      "\u001b[35mlayer_outputs = layer_module(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[35mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 509, in forward\u001b[0m\n",
      "\u001b[35mself_attention_outputs = self.attention(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[35mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 443, in forward\u001b[0m\n",
      "\u001b[35mself_outputs = self.self(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[35mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 331, in forward\u001b[0m\n",
      "\u001b[35mquery_layer = query_layer * self.attention_head_size**-0.5\u001b[0m\n",
      "\u001b[35mtorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 3; 31.74 GiB total capacity; 30.86 GiB already allocated; 21.38 MiB free; 30.87 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\u001b[0m\n",
      "\u001b[35mTraceback (most recent call last):\n",
      "  File \"/opt/ml/code/cuda-uniref100-pretorkenized-mlm-train-ddp-fsdp.py\", line 365, in <module>\u001b[0m\n",
      "\u001b[35moutputs = model(**batch)  # Forward pass\u001b[0m\n",
      "\u001b[35mFile \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[35mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py\", line 2727, in forward\u001b[0m\n",
      "\u001b[35moutput = self._fsdp_wrapped_module(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[35mreturn forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[35mFile \"/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/flatten_params_wrapper.py\", line 165, in forward\u001b[0m\n",
      "\u001b[35mreturn self.module(*inputs, **kwinputs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[35mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 1017, in forward\u001b[0m\n",
      "\u001b[35moutputs = self.esm(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[35mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 923, in forward\u001b[0m\n",
      "\u001b[35mencoder_outputs = self.encoder(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[35mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 624, in forward\u001b[0m\n",
      "\u001b[35mlayer_outputs = layer_module(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[35mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 509, in forward\u001b[0m\n",
      "\u001b[35mself_attention_outputs = self.attention(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[35mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 443, in forward\u001b[0m\n",
      "\u001b[35mself_outputs = self.self(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[35mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 323, in forward\u001b[0m\n",
      "\u001b[35mvalue_layer = self.transpose_for_scores(self.value(hidden_states))\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\u001b[0m\n",
      "\u001b[35mreturn forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/linear.py\", line 114, in forward\u001b[0m\n",
      "\u001b[35mreturn F.linear(input, self.weight, self.bias)\u001b[0m\n",
      "\u001b[35mtorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 0; 31.74 GiB total capacity; 30.84 GiB already allocated; 27.38 MiB free; 30.84 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\u001b[0m\n",
      "\u001b[34m0%|          | 1/14854 [00:06<26:12:22,  6.35s/it]\u001b[0m\n",
      "\u001b[34mWARNING:torch.distributed.elastic.multiprocessing.api:Sending process 206 closing signal SIGTERM\u001b[0m\n",
      "\u001b[34mWARNING:torch.distributed.elastic.multiprocessing.api:Sending process 207 closing signal SIGTERM\u001b[0m\n",
      "\u001b[34mWARNING:torch.distributed.elastic.multiprocessing.api:Sending process 209 closing signal SIGTERM\u001b[0m\n",
      "\u001b[35mWARNING:torch.distributed.elastic.multiprocessing.api:Sending process 201 closing signal SIGTERM\u001b[0m\n",
      "\u001b[35mWARNING:torch.distributed.elastic.multiprocessing.api:Sending process 202 closing signal SIGTERM\u001b[0m\n",
      "\u001b[35mWARNING:torch.distributed.elastic.multiprocessing.api:Sending process 205 closing signal SIGTERM\u001b[0m\n",
      "\u001b[35mWARNING:torch.distributed.elastic.multiprocessing.api:Sending process 206 closing signal SIGTERM\u001b[0m\n",
      "\u001b[35mWARNING:torch.distributed.elastic.multiprocessing.api:Sending process 207 closing signal SIGTERM\u001b[0m\n",
      "\u001b[35mWARNING:torch.distributed.elastic.multiprocessing.api:Sending process 208 closing signal SIGTERM\u001b[0m\n",
      "\u001b[34mERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 202) of binary: /opt/conda/bin/python3.9\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"/opt/conda/bin/torchrun\", line 8, in <module>\u001b[0m\n",
      "\u001b[34msys.exit(main())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 346, in wrapper\u001b[0m\n",
      "\u001b[34mreturn f(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/distributed/run.py\", line 762, in main\u001b[0m\n",
      "\u001b[34mrun(args)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/distributed/run.py\", line 753, in run\u001b[0m\n",
      "\u001b[34melastic_launch(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 132, in __call__\u001b[0m\n",
      "\u001b[34mreturn launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 246, in launch_agent\u001b[0m\n",
      "\u001b[34mraise ChildFailedError(\u001b[0m\n",
      "\u001b[34mtorch.distributed.elastic.multiprocessing.errors\u001b[0m\n",
      "\u001b[34m.ChildFailedError: \u001b[0m\n",
      "\u001b[34m============================================================\u001b[0m\n",
      "\u001b[34mcuda-uniref100-pretorkenized-mlm-train-ddp-fsdp.py FAILED\u001b[0m\n",
      "\u001b[34m------------------------------------------------------------\u001b[0m\n",
      "\u001b[34mFailures:\u001b[0m\n",
      "\u001b[34m[1]:\n",
      "  time      : 2023-10-13_04:43:38\n",
      "  host      : algo-1\n",
      "  rank      : 1 (local_rank: 1)\n",
      "  exitcode  : 1 (pid: 203)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\u001b[0m\n",
      "\u001b[34m[2]:\n",
      "  time      : 2023-10-13_04:43:38\n",
      "  host      : algo-1\n",
      "  rank      : 2 (local_rank: 2)\n",
      "  exitcode  : 1 (pid: 204)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\u001b[0m\n",
      "\u001b[34m[3]:\n",
      "  time      : 2023-10-13_04:43:38\n",
      "  host      : algo-1\n",
      "  rank      : 3 (local_rank: 3)\n",
      "  exitcode  : 1 (pid: 205)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\u001b[0m\n",
      "\u001b[34m[4]:\n",
      "  time      : 2023-10-13_04:43:38\n",
      "  host      : algo-1\n",
      "  rank      : 6 (local_rank: 6)\n",
      "  exitcode  : 1 (pid: 208)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\u001b[0m\n",
      "\u001b[34m------------------------------------------------------------\u001b[0m\n",
      "\u001b[34mRoot Cause (first observed failure):\u001b[0m\n",
      "\u001b[34m[0]:\n",
      "  time      : 2023-10-13_04:43:38\n",
      "  host      : algo-1\n",
      "  rank      : 0 (local_rank: 0)\n",
      "  exitcode  : 1 (pid: 202)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\u001b[0m\n",
      "\u001b[34m============================================================\u001b[0m\n",
      "\u001b[34m2023-10-13 04:43:39,245 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-10-13 04:43:39,245 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 1 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-10-13 04:43:39,246 sagemaker-training-toolkit ERROR    Reporting training FAILURE\u001b[0m\n",
      "\u001b[34m2023-10-13 04:43:39,247 sagemaker-training-toolkit ERROR    ExecuteUserScriptError:\u001b[0m\n",
      "\u001b[34mExitCode 1\u001b[0m\n",
      "\u001b[34mErrorMessage \"torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 2; 31.74 GiB total capacity; 30.89 GiB already allocated; 15.38 MiB free; 30.90 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      " Traceback (most recent call last)\n",
      " File \"/opt/ml/code/cuda-uniref100-pretorkenized-mlm-train-ddp-fsdp.py\", line 365, in <module>\n",
      " outputs = model(**batch)  # Forward pass\n",
      " File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      " return forward_call(*input, **kwargs)\n",
      " File \"/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py\", line 2727, in forward\n",
      " output = self._fsdp_wrapped_module(*args, **kwargs)\n",
      " File \"/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/flatten_params_wrapper.py\", line 165, in forward\n",
      " return self.module(*inputs, **kwinputs)\n",
      " File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 1017, in forward\n",
      " outputs = self.esm(\n",
      " File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 923, in forward\n",
      " encoder_outputs = self.encoder(\n",
      " File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 624, in forward\n",
      " layer_outputs = layer_module(\n",
      " File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 509, in forward\n",
      " self_attention_outputs = self.attention(\n",
      " File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 443, in forward\n",
      " self_outputs = self.self(\n",
      " File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 331, in forward\n",
      " query_layer = query_layer * self.attention_head_size**-0.5\n",
      " torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 3; 31.74 GiB total capacity; 30.86 GiB already allocated; 21.38 MiB free; 30.87 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      " File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 323, in forward\n",
      " value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
      " File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/linear.py\", line 114, in forward\n",
      " return F.linear(input, self.weight, self.bias)\n",
      " torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 4; 31.74 GiB total capacity; 30.84 GiB already allocated; 7.38 MiB free; 30.84 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      " torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 0; 31.74 GiB total capacity; 30.84 GiB already allocated; 27.38 MiB free; 30.84 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      " File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 551, in forward\n",
      " layer_output = self.feed_forward_chunk(attention_output)\n",
      " File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 562, in feed_forward_chunk\n",
      " intermediate_output = self.intermediate(attention_output_ln)\n",
      " File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 464, in forward\n",
      " hidden_states = gelu(hidden_states)\n",
      " File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 67, in gelu\n",
      " return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
      " torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 120.00 MiB (GPU 1; 31.74 GiB total capacity; 30.72 GiB already allocated; 111.38 MiB free; 30.72 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      " torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 31.74 GiB total capacity; 30.86 GiB already allocated; 5.38 MiB free; 30.87 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      " torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 7; 31.74 GiB total capacity; 30.86 GiB already allocated; 29.38 MiB free; 30.87 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      " torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 5; 31.74 GiB total capacity; 30.86 GiB already allocated; 21.38 MiB free; 30.87 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      " 0%|          | 1/14854 [00:06<26:12:22,  6.35s/it]\n",
      " WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 206 closing signal SIGTERM\n",
      " WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 207 closing signal SIGTERM\n",
      " WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 209 closing signal SIGTERM\n",
      " ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 202) of binary: /opt/conda/bin/python3.9\n",
      " File \"/opt/conda/bin/torchrun\", line 8, in <module>\n",
      " sys.exit(main())\n",
      " File \"/opt/conda/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 346, in wrapper\n",
      " return f(*args, **kwargs)\n",
      " File \"/opt/conda/lib/python3.9/site-packages/torch/distributed/run.py\", line 762, in main\n",
      " run(args)\n",
      " File \"/opt/conda/lib/python3.9/site-packages/torch/distributed/run.py\", line 753, in run\n",
      " elastic_launch(\n",
      " File \"/opt/conda/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 132, in __call__\n",
      " return launch_agent(self._config, self._entrypoint, list(args))\n",
      " File \"/opt/conda/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 246, in launch_agent\n",
      " raise ChildFailedError(\n",
      " torch.distributed.elastic.multiprocessing.errors\n",
      " .ChildFailedError\n",
      " ============================================================\n",
      " cuda-uniref100-pretorkenized-mlm-train-ddp-fsdp.py FAILED\n",
      " ------------------------------------------------------------\n",
      " Failures\n",
      " [1]\n",
      " time      : 2023-10-13_04:43:38\n",
      " host      : algo-1\n",
      " rank      : 1 (local_rank: 1)\n",
      " exitcode  : 1 (pid: 203)\n",
      " error_file: <N/A>\n",
      " traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      " [2]\n",
      " rank      : 2 (local_rank: 2)\n",
      " exitcode  : 1 (pid: 204)\n",
      " [3]\n",
      " rank      : 3 (local_rank: 3)\n",
      " exitcode  : 1 (pid: 205)\n",
      " [4]\n",
      " rank      : 6 (local_rank: 6)\n",
      " exitcode  : 1 (pid: 208)\n",
      " Root Cause (first observed failure)\n",
      " [0]\n",
      " rank      : 0 (local_rank: 0)\n",
      " exitcode  : 1 (pid: 202)\"\u001b[0m\n",
      "\u001b[34mCommand \"torchrun --nnodes 2 --nproc_per_node 8 --master_addr algo-1 --master_port 7777 --node_rank 0 cuda-uniref100-pretorkenized-mlm-train-ddp-fsdp.py --bf16 True --logging_steps 8 --model_id facebook/esm2_t33_650M_UR50D --num_train_epochs 2 --optim adamw_torch --per_device_eval_batch_size 12 --per_device_train_batch_size 12 --pretrain 1 --test_index_file_path sample_test_index_map --train_index_file_path sample_train_index_map --train_sample_count 10000\"\u001b[0m\n",
      "\u001b[34m2023-10-13 04:43:39,247 sagemaker-training-toolkit ERROR    Encountered exit_code 1\u001b[0m\n",
      "\u001b[35mERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 2 (pid: 203) of binary: /opt/conda/bin/python3.9\u001b[0m\n",
      "\u001b[35mTraceback (most recent call last):\n",
      "  File \"/opt/conda/bin/torchrun\", line 8, in <module>\u001b[0m\n",
      "\u001b[35msys.exit(main())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 346, in wrapper\u001b[0m\n",
      "\u001b[35mreturn f(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/distributed/run.py\", line 762, in main\u001b[0m\n",
      "\u001b[35mrun(args)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/distributed/run.py\", line 753, in run\u001b[0m\n",
      "\u001b[35melastic_launch(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 132, in __call__\u001b[0m\n",
      "\u001b[35mreturn launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 246, in launch_agent\u001b[0m\n",
      "\u001b[35mraise ChildFailedError(\u001b[0m\n",
      "\u001b[35mtorch.distributed.elastic.multiprocessing.errors.ChildFailedError: \u001b[0m\n",
      "\u001b[35m============================================================\u001b[0m\n",
      "\u001b[35mcuda-uniref100-pretorkenized-mlm-train-ddp-fsdp.py FAILED\u001b[0m\n",
      "\u001b[35m------------------------------------------------------------\u001b[0m\n",
      "\u001b[35mFailures:\u001b[0m\n",
      "\u001b[35m[1]:\n",
      "  time      : 2023-10-13_04:43:38\n",
      "  host      : algo-2\n",
      "  rank      : 11 (local_rank: 3)\n",
      "  exitcode  : 1 (pid: 204)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\u001b[0m\n",
      "\u001b[35m------------------------------------------------------------\u001b[0m\n",
      "\u001b[35mRoot Cause (first observed failure):\u001b[0m\n",
      "\u001b[35m[0]:\n",
      "  time      : 2023-10-13_04:43:38\n",
      "  host      : algo-2\n",
      "  rank      : 10 (local_rank: 2)\n",
      "  exitcode  : 1 (pid: 203)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\u001b[0m\n",
      "\u001b[35m============================================================\u001b[0m\n",
      "\u001b[35m2023-10-13 04:43:39,204 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[35m2023-10-13 04:43:39,204 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 1 from exiting process.\u001b[0m\n",
      "\u001b[35m2023-10-13 04:43:39,205 sagemaker-training-toolkit ERROR    Reporting training FAILURE\u001b[0m\n",
      "\u001b[35m2023-10-13 04:43:39,205 sagemaker-training-toolkit ERROR    ExecuteUserScriptError:\u001b[0m\n",
      "\u001b[35mExitCode 1\u001b[0m\n",
      "\u001b[35mErrorMessage \"torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 2; 31.74 GiB total capacity; 30.89 GiB already allocated; 15.38 MiB free; 30.90 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      " Traceback (most recent call last)\n",
      " File \"/opt/ml/code/cuda-uniref100-pretorkenized-mlm-train-ddp-fsdp.py\", line 365, in <module>\n",
      " outputs = model(**batch)  # Forward pass\n",
      " File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      " return forward_call(*input, **kwargs)\n",
      " File \"/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py\", line 2727, in forward\n",
      " output = self._fsdp_wrapped_module(*args, **kwargs)\n",
      " File \"/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/flatten_params_wrapper.py\", line 165, in forward\n",
      " return self.module(*inputs, **kwinputs)\n",
      " File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 1017, in forward\n",
      " outputs = self.esm(\n",
      " File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 923, in forward\n",
      " encoder_outputs = self.encoder(\n",
      " File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 624, in forward\n",
      " layer_outputs = layer_module(\n",
      " File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 509, in forward\n",
      " self_attention_outputs = self.attention(\n",
      " File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 443, in forward\n",
      " self_outputs = self.self(\n",
      " File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 323, in forward\n",
      " value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
      " File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/linear.py\", line 114, in forward\n",
      " return F.linear(input, self.weight, self.bias)\n",
      " torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 4; 31.74 GiB total capacity; 30.84 GiB already allocated; 7.38 MiB free; 30.84 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      " \n",
      " File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 331, in forward\n",
      " query_layer = query_layer * self.attention_head_size**-0.5\n",
      " torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 5; 31.74 GiB total capacity; 30.86 GiB already allocated; 21.38 MiB free; 30.87 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      " File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 551, in forward\n",
      " layer_output = self.feed_forward_chunk(attention_output)\n",
      " File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 562, in feed_forward_chunk\n",
      " intermediate_output = self.intermediate(attention_output_ln)\n",
      " File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 464, in forward\n",
      " hidden_states = gelu(hidden_states)\n",
      " File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 67, in gelu\n",
      " return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
      " torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 120.00 MiB (GPU 1; 31.74 GiB total capacity; 30.72 GiB already allocated; 111.38 MiB free; 30.72 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      " outputs = model(**batch)  # Forward passoutputs = model(**batch)  # Forward pass\n",
      " return forward_call(*input, **kwargs)return forward_call(*input, **kwargs)\n",
      " output = self._fsdp_wrapped_module(*args, **kwargs)output = self._fsdp_wrapped_module(*args, **kwargs)\n",
      " return forward_call(*input, **kwargs)  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 923, in forward\n",
      " encoder_outputs = self.encoder(encoder_outputs = self.encoder(\n",
      " layer_outputs = layer_module(layer_outputs = layer_module(\n",
      " torch.cuda.OutOfMemoryError    : return forward_call(*input, **kwargs)\n",
      " CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 31.74 GiB total capacity; 30.86 GiB already allocated; 5.38 MiB free; 30.87 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py\", line 331, in forward\n",
      " torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 7; 31.74 GiB total capacity; 30.86 GiB already allocated; 23.38 MiB free; 30.87 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      " torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 3; 31.74 GiB total capacity; 30.86 GiB already allocated; 21.38 MiB free; 30.87 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      " torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 0; 31.74 GiB total capacity; 30.84 GiB already allocated; 27.38 MiB free; 30.84 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      " WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 201 closing signal SIGTERM\n",
      " WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 202 closing signal SIGTERM\n",
      " WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 205 closing signal SIGTERM\n",
      " WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 206 closing signal SIGTERM\n",
      " WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 207 closing signal SIGTERM\n",
      " WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 208 closing signal SIGTERM\n",
      " ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 2 (pid: 203) of binary: /opt/conda/bin/python3.9\n",
      " File \"/opt/conda/bin/torchrun\", line 8, in <module>\n",
      " sys.exit(main())\n",
      " File \"/opt/conda/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 346, in wrapper\n",
      " return f(*args, **kwargs)\n",
      " File \"/opt/conda/lib/python3.9/site-packages/torch/distributed/run.py\", line 762, in main\n",
      " run(args)\n",
      " File \"/opt/conda/lib/python3.9/site-packages/torch/distributed/run.py\", line 753, in run\n",
      " elastic_launch(\n",
      " File \"/opt/conda/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 132, in __call__\n",
      " return launch_agent(self._config, self._entrypoint, list(args))\n",
      " File \"/opt/conda/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 246, in launch_agent\n",
      " raise ChildFailedError(\n",
      " torch.distributed.elastic.multiprocessing.errors.ChildFailedError\n",
      " ============================================================\n",
      " cuda-uniref100-pretorkenized-mlm-train-ddp-fsdp.py FAILED\n",
      " ------------------------------------------------------------\n",
      " Failures\n",
      " [1]\n",
      " time      : 2023-10-13_04:43:38\n",
      " host      : algo-2\n",
      " rank      : 11 (local_rank: 3)\n",
      " exitcode  : 1 (pid: 204)\n",
      " error_file: <N/A>\n",
      " traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      " Root Cause (first observed failure)\n",
      " [0]\n",
      " rank      : 10 (local_rank: 2)\n",
      " exitcode  : 1 (pid: 203)\"\u001b[0m\n",
      "\u001b[35mCommand \"torchrun --nnodes 2 --nproc_per_node 8 --master_addr algo-1 --master_port 7777 --node_rank 1 cuda-uniref100-pretorkenized-mlm-train-ddp-fsdp.py --bf16 True --logging_steps 8 --model_id facebook/esm2_t33_650M_UR50D --num_train_epochs 2 --optim adamw_torch --per_device_eval_batch_size 12 --per_device_train_batch_size 12 --pretrain 1 --test_index_file_path sample_test_index_map --train_index_file_path sample_train_index_map --train_sample_count 10000\"\u001b[0m\n",
      "\u001b[35m2023-10-13 04:43:39,205 sagemaker-training-toolkit ERROR    Encountered exit_code 1\u001b[0m\n",
      "\n",
      "2023-10-13 04:44:09 Uploading - Uploading generated training model\n",
      "2023-10-13 04:44:09 Failed - Training job failed\n"
     ]
    },
    {
     "ename": "ClientError",
     "evalue": "An error occurred (ValidationException) when calling the UpdateTrialComponent operation: 2 validation errors detected: Value 'Error for Training job esm-2-uniref100-2p3dn24-2023-10-13-04-37-07-449: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nExitCode 1\nErrorMessage \"torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 2; 31.74 GiB total capacity; 30.89 GiB already allocated; 15.38 MiB free; 30.90 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n Traceback (most recent call last)\n File \"/opt/ml/code/cuda-uniref100-pretorkenized-mlm-train-ddp-fsdp.py\", line 365, in <module>\n outputs = model(**batch)  # Forward pass\n File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n return forward_call(*input, **kwargs)\n File \"/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py\", line 2727, in forward\n output = self._fsdp_wrapped_module(*args, **kwargs)\n File \"/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/flatten_params_wrapper.py\", line 165, in ' at 'status.message' failed to satisfy constraint: Member must have length less than or equal to 1024; Value 'Error for Training job esm-2-uniref100-2p3dn24-2023-10-13-04-37-07-449: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nExitCode 1\nErrorMessage \"torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 2; 31.74 GiB total capacity; 30.89 GiB already allocated; 15.38 MiB free; 30.90 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n Traceback (most recent call last)\n File \"/opt/ml/code/cuda-uniref100-pretorkenized-mlm-train-ddp-fsdp.py\", line 365, in <module>\n outputs = model(**batch)  # Forward pass\n File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n return forward_call(*input, **kwargs)\n File \"/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py\", line 2727, in forward\n output = self._fsdp_wrapped_module(*args, **kwargs)\n File \"/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/flatten_params_wrapper.py\", line 165, in ' at 'status.message' failed to satisfy constraint: Member must satisfy regular expression pattern: .*",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[82], line 37\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Run(\n\u001b[1;32m     34\u001b[0m     experiment_name\u001b[38;5;241m=\u001b[39mEXPERIMENT_NAME,\n\u001b[1;32m     35\u001b[0m     sagemaker_session\u001b[38;5;241m=\u001b[39msagemaker_session,\n\u001b[1;32m     36\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m run:\n\u001b[0;32m---> 37\u001b[0m     \u001b[43mg5_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mTrainingInput\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms3_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_s3_uri_uniref100\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFastFile\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mTrainingInput\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms3_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_s3_uri_uniref100\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFastFile\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/workflow/pipeline_context.py:311\u001b[0m, in \u001b[0;36mrunnable_by_pipeline.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _StepArguments(retrieve_caller_name(self_instance), run_func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/estimator.py:1314\u001b[0m, in \u001b[0;36mEstimatorBase.fit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m   1313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[0;32m-> 1314\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlatest_training_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/estimator.py:2597\u001b[0m, in \u001b[0;36m_TrainingJob.wait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m   2596\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logs \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 2597\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msagemaker_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogs_for_job\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjob_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2598\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/session.py:4963\u001b[0m, in \u001b[0;36mSession.logs_for_job\u001b[0;34m(self, job_name, wait, poll, log_type, timeout)\u001b[0m\n\u001b[1;32m   4943\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Display logs for a given training job, optionally tailing them until job is complete.\u001b[39;00m\n\u001b[1;32m   4944\u001b[0m \n\u001b[1;32m   4945\u001b[0m \u001b[38;5;124;03mIf the output is a tty or a Jupyter cell, it will be color-coded\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4961\u001b[0m \u001b[38;5;124;03m    exceptions.UnexpectedStatusException: If waiting and the training job fails.\u001b[39;00m\n\u001b[1;32m   4962\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m-> 4963\u001b[0m \u001b[43m_logs_for_job\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mboto_session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjob_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpoll\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/session.py:6887\u001b[0m, in \u001b[0;36m_logs_for_job\u001b[0;34m(boto_session, job_name, wait, poll, log_type, timeout)\u001b[0m\n\u001b[1;32m   6886\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[0;32m-> 6887\u001b[0m     \u001b[43m_check_job_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjob_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTrainingJobStatus\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6888\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dot:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/session.py:6940\u001b[0m, in \u001b[0;36m_check_job_status\u001b[0;34m(job, desc, status_key_name)\u001b[0m\n\u001b[1;32m   6935\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mCapacityError(\n\u001b[1;32m   6936\u001b[0m         message\u001b[38;5;241m=\u001b[39mmessage,\n\u001b[1;32m   6937\u001b[0m         allowed_statuses\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStopped\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   6938\u001b[0m         actual_status\u001b[38;5;241m=\u001b[39mstatus,\n\u001b[1;32m   6939\u001b[0m     )\n\u001b[0;32m-> 6940\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mUnexpectedStatusException(\n\u001b[1;32m   6941\u001b[0m     message\u001b[38;5;241m=\u001b[39mmessage,\n\u001b[1;32m   6942\u001b[0m     allowed_statuses\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStopped\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   6943\u001b[0m     actual_status\u001b[38;5;241m=\u001b[39mstatus,\n\u001b[1;32m   6944\u001b[0m )\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error for Training job esm-2-uniref100-2p3dn24-2023-10-13-04-37-07-449: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nExitCode 1\nErrorMessage \"torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 2; 31.74 GiB total capacity; 30.89 GiB already allocated; 15.38 MiB free; 30.90 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n Traceback (most recent call last)\n File \"/opt/ml/code/cuda-uniref100-pretorkenized-mlm-train-ddp-fsdp.py\", line 365, in <module>\n outputs = model(**batch)  # Forward pass\n File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n return forward_call(*input, **kwargs)\n File \"/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py\", line 2727, in forward\n output = self._fsdp_wrapped_module(*args, **kwargs)\n File \"/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/flatten_params_wrapper.py\", line 165, in ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[82], line 33\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# creates Hugging Face estimator\u001b[39;00m\n\u001b[1;32m     17\u001b[0m g5_estimator \u001b[38;5;241m=\u001b[39m PyTorch(\n\u001b[1;32m     18\u001b[0m     base_job_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mesm-2-uniref100-2p3dn24\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     19\u001b[0m     entry_point\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda-uniref100-pretorkenized-mlm-train-ddp-fsdp.py\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m     tags\u001b[38;5;241m=\u001b[39m[{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKey\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproject\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValue\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mesm-benchmarking\u001b[39m\u001b[38;5;124m\"\u001b[39m}],\n\u001b[1;32m     31\u001b[0m )\n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Run(\n\u001b[1;32m     34\u001b[0m     experiment_name\u001b[38;5;241m=\u001b[39mEXPERIMENT_NAME,\n\u001b[1;32m     35\u001b[0m     sagemaker_session\u001b[38;5;241m=\u001b[39msagemaker_session,\n\u001b[1;32m     36\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m run:\n\u001b[1;32m     37\u001b[0m     g5_estimator\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m     38\u001b[0m         {\n\u001b[1;32m     39\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m: TrainingInput(s3_data\u001b[38;5;241m=\u001b[39mtrain_s3_uri_uniref100, input_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFastFile\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     42\u001b[0m         wait\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     43\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/experiments/run.py:757\u001b[0m, in \u001b[0;36mRun.__exit__\u001b[0;34m(self, exc_type, exc_value, exc_traceback)\u001b[0m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    753\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trial_component\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m _api_types\u001b[38;5;241m.\u001b[39mTrialComponentStatus(\n\u001b[1;32m    754\u001b[0m         primary_status\u001b[38;5;241m=\u001b[39m_TrialComponentStatusType\u001b[38;5;241m.\u001b[39mCompleted\u001b[38;5;241m.\u001b[39mvalue\n\u001b[1;32m    755\u001b[0m     )\n\u001b[0;32m--> 757\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/experiments/run.py:537\u001b[0m, in \u001b[0;36mRun.close\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Persist any data saved locally.\"\"\"\u001b[39;00m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    536\u001b[0m     \u001b[38;5;66;03m# Update the trial component with additions from the Run object\u001b[39;00m\n\u001b[0;32m--> 537\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_trial_component\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;66;03m# Create Lineage entities for the artifacts\u001b[39;00m\n\u001b[1;32m    539\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lineage_artifact_tracker\u001b[38;5;241m.\u001b[39msave()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/experiments/trial_component.py:121\u001b[0m, in \u001b[0;36m_TrialComponent.save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    120\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Save the state of this TrialComponent to SageMaker.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_invoke_api\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_boto_update_method\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_boto_update_members\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/apiutils/_base_types.py:226\u001b[0m, in \u001b[0;36mRecord._invoke_api\u001b[0;34m(self, boto_method, boto_method_members)\u001b[0m\n\u001b[1;32m    224\u001b[0m api_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_boto(api_values)\n\u001b[1;32m    225\u001b[0m api_method \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msagemaker_session\u001b[38;5;241m.\u001b[39msagemaker_client, boto_method)\n\u001b[0;32m--> 226\u001b[0m api_boto_response \u001b[38;5;241m=\u001b[39m \u001b[43mapi_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mapi_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwith_boto(api_boto_response)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/botocore/client.py:535\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    531\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    532\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpy_operation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() only accepts keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    533\u001b[0m     )\n\u001b[1;32m    534\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 535\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/botocore/client.py:980\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    978\u001b[0m     error_code \u001b[38;5;241m=\u001b[39m parsed_response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCode\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    979\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m--> 980\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m    981\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    982\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parsed_response\n",
      "\u001b[0;31mClientError\u001b[0m: An error occurred (ValidationException) when calling the UpdateTrialComponent operation: 2 validation errors detected: Value 'Error for Training job esm-2-uniref100-2p3dn24-2023-10-13-04-37-07-449: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nExitCode 1\nErrorMessage \"torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 2; 31.74 GiB total capacity; 30.89 GiB already allocated; 15.38 MiB free; 30.90 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n Traceback (most recent call last)\n File \"/opt/ml/code/cuda-uniref100-pretorkenized-mlm-train-ddp-fsdp.py\", line 365, in <module>\n outputs = model(**batch)  # Forward pass\n File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n return forward_call(*input, **kwargs)\n File \"/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py\", line 2727, in forward\n output = self._fsdp_wrapped_module(*args, **kwargs)\n File \"/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/flatten_params_wrapper.py\", line 165, in ' at 'status.message' failed to satisfy constraint: Member must have length less than or equal to 1024; Value 'Error for Training job esm-2-uniref100-2p3dn24-2023-10-13-04-37-07-449: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nExitCode 1\nErrorMessage \"torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 2; 31.74 GiB total capacity; 30.89 GiB already allocated; 15.38 MiB free; 30.90 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n Traceback (most recent call last)\n File \"/opt/ml/code/cuda-uniref100-pretorkenized-mlm-train-ddp-fsdp.py\", line 365, in <module>\n outputs = model(**batch)  # Forward pass\n File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n return forward_call(*input, **kwargs)\n File \"/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py\", line 2727, in forward\n output = self._fsdp_wrapped_module(*args, **kwargs)\n File \"/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/flatten_params_wrapper.py\", line 165, in ' at 'status.message' failed to satisfy constraint: Member must satisfy regular expression pattern: .*"
     ]
    }
   ],
   "source": [
    "# Additional training parameters\n",
    "hyperparameters = {\n",
    "    \"num_train_epochs\": 2,\n",
    "    \"model_id\": MODEL_ID,\n",
    "    \"per_device_train_batch_size\": 12,\n",
    "    \"per_device_eval_batch_size\": 12,\n",
    "    \"bf16\": True,\n",
    "    \"logging_steps\": 8,\n",
    "    \"optim\": \"adamw_torch\",\n",
    "    \"pretrain\" : 1,\n",
    "    \"train_sample_count\" : 10000,\n",
    "    \"train_index_file_path\" : \"sample_train_index_map\",\n",
    "    \"test_index_file_path\" : \"sample_test_index_map\"\n",
    "}\n",
    "\n",
    "# creates Hugging Face estimator\n",
    "g5_estimator = PyTorch(\n",
    "    base_job_name=\"esm-2-uniref100-2p3dn24\",\n",
    "    entry_point=\"cuda-uniref100-pretorkenized-mlm-train-ddp-fsdp.py\",\n",
    "    source_dir=\"scripts/training/cuda/uniref100\",\n",
    "    instance_type=\"ml.p3dn.24xlarge\",\n",
    "    instance_count=2,\n",
    "    image_uri=f\"763104351884.dkr.ecr.{REGION_NAME}.amazonaws.com/pytorch-training:1.13.1-gpu-py39-cu117-ubuntu20.04-sagemaker\",\n",
    "    output_path=f\"{S3_PATH}/output\",\n",
    "    role=sagemaker_execution_role,\n",
    "    hyperparameters=hyperparameters,\n",
    "    metric_definitions=metric_definitions,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    distribution={\"torch_distributed\": {\"enabled\": True}},\n",
    "    tags=[{\"Key\": \"project\", \"Value\": \"esm-benchmarking\"}],\n",
    ")\n",
    "\n",
    "with Run(\n",
    "    experiment_name=EXPERIMENT_NAME,\n",
    "    sagemaker_session=sagemaker_session,\n",
    ") as run:\n",
    "    g5_estimator.fit(\n",
    "        {\n",
    "            \"train\": TrainingInput(s3_data=train_s3_uri_uniref100, input_mode=\"FastFile\"),\n",
    "            \"test\": TrainingInput(s3_data=test_s3_uri_uniref100, input_mode=\"FastFile\"),\n",
    "        },\n",
    "        wait=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9318ee-15ce-4ff3-b840-827c68e0b84a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.m5.large",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
